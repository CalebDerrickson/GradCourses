\input{definitions}

% Enter the specific assignment number and topic of that assignment below, and replace "Your Name" with your actual name.
\title{STAT 31210: Homework 7}
\author{Caleb Derrickson}
\date{February 23, 2024}

\begin{document}
\onehalfspacing
\maketitle
\allowdisplaybreaks
{\color{cit}\vspace{2mm}\noindent\textbf{Collaborators:}} The TA's of the class, as well as Kevin Hefner, and Alexander Cram.

\tableofcontents

\newpage
\section{Problem 8.12}
Suppose that $A: \scH \into \scH$ is a bounded, self-adjoint, linear operator such that there is a constant $c > 0$ with 
\[c \norm{x} \leq \norm{Ax} \quad \text{ for all } x \in \scH.\]
Prove that there is a unique solution $x$ of the equation $Ax = y$ for every $y \in \scH$.
\partbreak
\begin{solution}

    For this proof, we have to show two things: that there is a solution for all $x \in \scH$ and that solution is unique. First, we wish to show that $\range (A)$ is closed, which we can then apply Theorem 8.18, which will give us that $Ax$ has a solution for $y$ orthogonal to $\ker (A\star)$, which is $\ker(A)$ since $A$ is self-adjoint. To show $\range (A)$ is closed, take a Cauchy sequence $Ax_n \in\scH$. Then, by the property given of $A$, we have that 
    \[\norm{x_n - x_m} \leq \frac{1}{c}\norm{Ax_n - Ax_m}\]
    Since $Ax_n$ is Cauchy, we have that $\norm{x_n - x_m} \into 0$ as $n, m \into \infty$. Therefore, $x_n$ is a Cauchy sequence in $\scH$. Since $\scH$ is complete, then $x_n \into x \in \scH$, as $n \into \infty$. Note then that
    \[\norm{x_n - x} = \frac{\norm{A}}{\norm{A}}\norm{x_n - x} \geq \frac{1}{\norm{A}}\norm{Ax_n - Ax}\]
    The last step is an application of Cauchy Schwartz. The multiplication and division of the norm of $A$ can be done since $A$ is bounded (and assumed not the $0$-matrix)\footnote{Even if we assumed that $A$ could be the zero-matrix, then the solutions of $Ax$ would not be unique, therefore we exclude this case.}. The line above then implies that $\norm{Ax_n - Ax} \into 0$ as $n \into \infty$, since it is bounded by $\norm{x_n - x}$. Therefore, $x\in \range (A)$, so it is closed. By Theorem 8.18, we have that $Ax = y$ as solutions for all $y$ orthogonal to $\ker (A)$, since $A$ is self adjoint. I claim that $\ker (A) = \{0\}$, since if it weren't, then there would exist some $x \in \scH$ nonzero such that $Ax = 0$. But, 
    \[\norm{x} \leq c\norm{Ax} = 0 \implies \norm{x} = 0 \iff x = 0.\]
    Therfore, we see a contradiction on $x$, so $\ker (A) = \{0\}$. Therefore, $Ax = y$ has a solution for all $y \in \scH$, since all of $\scH$ is orthogonal to the zero set. Finally, we need to show that the solution is unique for all $x \in \scH$. Suppose there were $x_1, x_2 \in \scH$ such that $Ax_1 = Ax_2 = y$. But then $A(x_1 - x_2) = 0$, which implies that $x_1 - x_2 = 0$, since the kernel of $A$ is the zero set. Therefore, $x_1 = x_2$, so the solution is unique for all $x \in \scH$, proving the statement. 
\end{solution}

\newpage
\section{Problem 8.13}
Prove that an orthogonal set of vectors $\{u_\alpha : \alpha \in \scA \}$ in a Hilbert space $\scH$ is an orthonormal basis if and only if 
\[\sum_{\alpha \in \scA} u_\alpha \otimes u_\alpha = \id. \]
\partbreak
\begin{solution}
    I am most familiar with the notation used by physicists, so instead of $u_\alpha \otimes u_\alpha$, I will use $\ketbra{u_\alpha}{u_\alpha}$. For simplicity, denote $A$ as the operator given. 
    \begin{itemize}
        \item[] \underline{$\implies$}:
        
        \hop
        Here we will assume that $\{u_\alpha\}$ is an orthonormal basis, and I will show the forward implication holds. Let $x \in \scH$. Then by since $\{u_\alpha\}$ is an orthonormal subset of $\scH$ (it is an orthonormal basis for $\scH$ so this should hold), then by Theorem 6.26, we can write $x$ as 
        \[\ket{x} = \sum_{\alpha \in \scA} \braket{u_\alpha}{x} \ket{u_\alpha}\]
        By the definition of the orthogonal projection on page 190 in the book, for any $v, y \in \scH$, $(\ketbra{v}{v})\ket{y} = \braket{v}{y}\ket{v}$. Applying this, we have that
        \[\ket{x} = \sum_{\alpha \in \scA} (\ketbra{u_\alpha}{u_\alpha}) \ket{x}\]
        Therefore, in terms of $A$, we have that $x = Ax$, implying that $A = \id$. This then shows the forward implication. 

        \item[] \underline{$\impliedby$}: 

        \hop
        Suppose false. That is, $\{u_\alpha\}$ is not an orthonormal set (still orthogonal, just $\norm{u_\alpha} \neq 1$), yet 
        \[\sum_{\alpha \in \scA} \ketbra{u_\alpha}{u_\alpha} = \id.\]
        In terms of matrix language, this would imply that for any $\alpha\in \scA$, 
        \[\ketbra{u_\alpha}{u_\alpha} = \ketbra{e_\alpha}{e_\alpha},\]
        where $e_\alpha$ is the standard basis vector. This implies  the rank one matrix generated by $\ketbra{u_\alpha}{u_\alpha}$ is the same as the rank one matrix generated by $\ketbra{e_\alpha}{e_\alpha}$. Note that both have only one nonzero element, the index $(\alpha, \alpha)$, since both are orthogonal. This implies that $\norm{u_\alpha}^2 = \norm{e_\alpha}^2 = 1$, therefore, $\norm{u_\alpha}^2 = \braket{u_\alpha}{u_\alpha} = 1$, which is a contradiction. Therefore, $\{u_\alpha\}$ is an orthonormal basis for $\scH$.    
    \end{itemize}
\end{solution}

\newpage
\section{Problem 8.14}
Suppose that $A, B \in \mfB (\scH)$ satisfy
\[\braket{x}{Ay} = \braket{x}{By} \quad \text{ for all } x, y \in \scH.\]
Prove that $A = B$. Use a polarization type identity to prove that if $\scH$ is a complex Hilbert space and 
\[\braket{x}{Ax} = \braket{x}{Bx} \quad \text{ for all } x\in \scH,\]
then $A = B$. What can you say about $A$ and $B$ for real Hilbert spaces?
\partbreak
\begin{solution}

    We will show that $A = B$ when $\braket{x}{Ay} = \braket{x}{By}$. Note that this implies $\braket{x}{(A - B)y} = 0$, therefore, $x \perp \range(A - B)$ for all $x \in \scH$. Since this is for all $x \in \scH$, then it must mean $\range(A - B) = \{0\}$, therefore, $A - B$ is the zero mapping. This implies that $A - B = 0$, so $A = B$. \par

    \hop
    In using a polarization identity, we can see when expanding out the inner product that 
    \[\braket{x}{Ax} = \frac{1}{4}\left[ \braket{x+Ax} - \braket{x - Ax} + i\braket{x + Ax} - i\braket{x - Ax}\right].\]
    Similarly, for $B$, 
    \[\braket{x}{Bx} = \frac{1}{4}\left[ \braket{x+Bx} - \braket{x - Bx} + i\braket{x + Bx} - i\braket{x - Bx}\right].\]
    Since $\braket{x}{Ax} = \braket{x}{Bx}$ for all $x$, we have that
    \begin{align*}
        &\braket{x}{Ax} - \braket{x}{Bx} = 0\\
        \implies & \braket{x + Ax} + \braket{x - Ax} - \braket{x + Bx} - \braket{x - Bx}\\
        & = i\braket{x - Ax} - i\braket{x + Ax} - i\braket{x + Bx} + i\braket{x - Bx}
    \end{align*}
    Since the right side is complex and the left side is real valued, we have that 
    \begin{align*}
        &\braket{x + Ax} + \braket{x - Ax} - \braket{x + Bx} - \braket{x - Bx} = 0\\
        &\braket{x - Ax} - \braket{x + Ax} - \braket{x + Bx} + \braket{x - Bx} = 0
    \end{align*}
    From the first line, we can group terms together to get
    \begin{align*}
        &\braket{x + Ax} - \braket{x + Bx} + \braket{x + Ax} - \braket{x - Bx} = 0\\
        &\braket{(A - B)x} + \braket{(B - A)x} = 0
    \end{align*}
    Therefore, $\norm{(A - B)x} = -\norm{(B - A)x}$ for all $x$. Since we are free to multiply by a negative 1 inside the norm, we have that 
    \[\norm{(A - B)x} = -\norm{(A - B)x}\]
    Therefore, $(A - B)x = 0$ for all $x$, meaning $A - B$ is the zero mapping, so $A = B$. This result only hinged on the real part, where the complex part is not necessary. So the $A$ and $B$ for real Hilber spaces must equal as well. 
\end{solution}

\newpage
\section{Problem 8.17}
Prove that strong convergence implies weak convergence. Also prove that strong and weak convergence are equivalent in a finite-dimensional Hilbert space.
\partbreak
\begin{solution}

    \begin{enumerate}
        \item[] \underline{Strong $\implies$ weak}:

        \hop
        Let $x, y \in \scH$, and suppose there exists a sequence $x_n$ which converges to $x$ strongly. Then, $\lim\limits_{n\to\infty} \norm{x_n - x}$ = 0. Let $\ep > 0$; there exists an $N \in \N$ such that when $n \geq N \implies \norm{x_n - x} < \frac{\ep}{\norm{y}}$ for $y \neq 0$. \footnote{If $y = 0$, then weak topology is already satisfied, since $\braket{x_n}{y} - \braket{x}{y} = 0 < \ep$.} Therefore, when analyzing the definition of the weak topology, we see that 
        \[|\braket{x_n}{y} - \braket{x}{y}| = |\braket{x_n - x}{y}| \leq \norm{x_n - x} \norm{y} < \frac{\ep}{\norm{y}}\norm{y} = \ep.\]
        This will hold for any $y \in \scH$, therefore, if $x_n \into x$ strongly, then $x_n \into x$ weakly. 

        \item[] \underline{Weak $\implies$ strong (in finite dimensions.)}:

        \hop
        Suppose $|\scH| = n$. Since any Hilbert space has an orthonormal basis, let $\{u_j\}_{j = 1}^{n}$ be such basis. Let $\ep > 0$; there exists an $N \in \N$ such that when $k \geq N \implies |\braket{x_k}{y} -\braket{x}{y}| < \frac{\ep}{\sqrt{n}}$ for any $y \in \scH$. By linearity of the inner product, we have
        \[|\braket{y}{x_k - x}| <\frac{\ep}{\sqrt{n}}.\]
        Since this should hold for any $y \in \scH$, it should hold then for the bases. Then $|\braket{u_j}{x_k - x}| <\frac{\ep}{\sqrt{n}}$ for any $u_j \in \{u_j\}_{j = 1}^n$. By Theorem 6.26, we have that 
        \[\norm{x_k - x}^2 = \sum_{j = 1}^n |\braket{u_j}{x_k - x}|^2.\]
        But since each term is less than $\frac{\ep}{n}$, 
        \[\norm{x_k - x}^2 = \sum_{j = 1}^n |\braket{u_j}{x_k - x}|^2 < \sum_{j = 1}^n \left(\frac{\ep}{\sqrt{n}}\right)^2 = \ep^2.\]
        Therefore, $\norm{x_k - x} < \ep$ for sufficiently large $k$, implying weak convergence is equivalent to strong convergence in a finite dimensional Hilbert space.
    \end{enumerate}
\end{solution}

\newpage
\section{Problem 8.18}
Let $u_n$ be a sequence of orthonormal vectors in a Hilbert space. Prove that $u_n \rightharpoonup 0$ weakly.  
\partbreak
\begin{solution}

    Let $y \in \scH$. To show that $u_n \rightharpoonup 0$ weakly, we need to show that $|\braket{u_n}{y}| \to 0$ as $n \to \infty.$ By Bessel's Inequality, we have that 
    \[\sum_{n = 1}^\infty |\braket{u_n}{y}|^2 \leq \norm{y}^2\]
    Since this is then a converging sum, we have that $|\braket{u_n}{y}|^2 \to 0$ as $n \into \infty$. This then implies that $|\braket{u_n}{y}| \to 0$ as $n \into \infty$, showing weak convergence.   
\end{solution}

\newpage
\section{Problem 8.20}
\newcommand{\xbar}{\Bar{x}}
Let $\scH$ be a real Hilbert space and $\ph \in \scH\star$. Define the quadratic functional $f: \scH \into \R$ by 
\[f(x) = \frac{1}{2}\norm{x}^2 - \ph(x).\]
Prove that there is a unique element $\xbar \in \scH$ such that 
\[f(\xbar) = \inf_{x \in \scH} f(x).\]
\partbreak
\begin{solution}

    Suppose $\xbar$ is the value at which the minimum is obtained. By the Riesz Representation Theorem, (Theorem 8.12), there exists a unique vector $x_0 \in \scH$ such that $\ph(x) = \braket{x}{x_0}$ for all $x \in \scH$. Then, since $\xbar$ is the minimum of $f$, we then have, for any $x \in \scH$,
    \[f(\xbar) \leq f(\xbar + x) \implies \frac{1}{2}\norm{\xbar}^2 - \braket{\xbar}{x_0}  \leq \frac{1}{2}\norm{\xbar + x}^2 - \braket{\xbar + x}{x_0}.\]
    By rearranging, we have that 
    \[\frac{1}{2}\norm{\xbar}^2  \leq \frac{1}{2}\norm{\xbar + x}^2 - \braket{x}{x_0}\]
    By using the definition of the norm on a Hilbert space, we can expand the norm to get
    \[\frac{1}{2}\norm{\xbar}^2  \leq \frac{1}{2}(\norm{x}^2 + 2\braket{\xbar}{x} + \norm{\xbar}^2) - \braket{x}{x_0}.\]
    Simplifying, we get 
    \[\braket{x_0 - \xbar}{x} \leq \frac{1}{2}\norm{x}^2 \ \forall \ x \in \scH.\]
    Set $x = x_0 - \xbar$. Then,
    \[\braket{x_0 - \xbar} \leq \frac{1}{2}\norm{x_0 - \xbar} \implies \frac{1}{2}\norm{x_0 - \xbar} \leq 0\]
    Since the norm function is positive, we have that $x_0 - \xbar = 0$, therefore, $x_0 = \xbar$. Since the function given is quadratic, its minimum is then uniquely attained. 
\end{solution}

\end{document}