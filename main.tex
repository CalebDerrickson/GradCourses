\documentclass[12pt]{article}
\usepackage[paper=letterpaper,margin=1.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{newtxtext, newtxmath}
\usepackage{enumitem}
\usepackage{titling}
\usepackage[colorlinks=true]{hyperref}
\usepackage{setspace}
\usepackage{braket}

\setlength{\droptitle}{-6em}

\newcommand{\hop}{\vspace{1mm}}
\newcommand{\jump}{\vspace{5mm}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\bt}{\textbf}
\newcommand{\lm}{\lambda}



% Enter the specific assignment number and topic of that assignment below, and replace "Your Name" with your actual name.
\title{STAT 30900: Homework 0}
\author{Caleb Derrickson}
\date{October 8, 2023}

\begin{document}
\onehalfspacing
\maketitle
\textbf{\underline{Collaborators:}} I had the help of Kevin Hefner and Alexander Cram,  as well as TA Andrew Dennehy on problems 5 and 6.
\begin{enumerate}[leftmargin=\labelsep]


\item 
\begin{enumerate}
    \hop
    \item For A $\in \R ^{m \times n}$, B $\in \R^{ n \times p}$, show that im(AB) $\subseteq$ im(A) and ker(B) $\subseteq$ ker(AB).
        \jump
        
    \begin{enumerate}
        \item Let \bt{z} $\in$ im(AB). We wish to show that \bt{z} $\in$ im(A). 
    \hop
    
    Since \bt{z} $\in$ im(AB), $\exists$ \bt{y} $\in$ $\R^{p}$ such that AB\bt{y} = \bt{z}. We can then say B\bt{y} $\in$ im(A), as $\exists$ a vector (namely, \bt{z}) where A(B\bt{y}) = \bt{z}. B\bt{y} $\in$ im(A), thus $\exists$ x $\in$ $\R^{m}$ where \bt{x} = B\bt{y}. Therefore, A\bt{x} = \bt{z}, meaning im(AB) $\subseteq$ im(A). $\square$

    Note that im(A) and im(AB) are equal when the kernel of B is empty, i.e. dim(B) = rank(B).
    
    \hop
    
    \item Let \textbf{z} $\in$ ker(B). We wish to show \textbf{z} $\in$ ker(AB).
    \hop

    Since \textbf{z} $\in$ ker(B), B\textbf{z} = 0. Applying the matrix A to both yields AB\textbf{z} = A0 = 0, so (AB)\textbf{z} = 0. Thus \textbf{z} $\in$ ker(AB), meaning ker(B) $\subseteq$ ker(AB). $\square$

    \hop
    Note that equality occurs when the kernel of A is empty, meaning no other vectors are mapped to the zero vector aside from the ones which were previously mapped by B.
    \end{enumerate}    

    \jump

    \item For A, B $\in$ $\R^{n\times n}$, show that
    \begin{enumerate}
        \item rank(AB) $\leq$ min \{rank(A), rank(B)\}, 
        \jump

        Note it is sufficient to show rank(AB) $\leq$ rank(A) and rank(AB) $\leq$ rank(B). The former follows directly from the first proof of the previous part, noting that im(AB) $\subseteq$ im(A) implies dim(im(AB)) $\leq$ dim(im(A)). The latter follows from the second proof of the first part. Using the rank-nullity theorem, we can rewrite rank(AB) as
        \[
        \text{rank(AB) = n - null(AB)}
        \]
        which from showing ker(B) $\subseteq$ ker(AB), implies that null(AB) $\geq$ null(B). Therefore,
        \[
        \text{rank(AB)} \leq \text{n - null(B)}
        \]
        which the left side can be rewritten as rank(B). Thus rank(AB) $\leq$ rank(B), meaning since rank(AB) is less than or equal to both ranks, it is less than the minimum of the two. $\square$
        \hop
        \item null(AB) $\leq$ null(A) + null(B), 
        \jump

        To prove this we will work backwards,
        \begin{align*}
        &\text{null(A) + null(B)} &\text{(Given.)}  \\
        =&\text{ n - rank(A) + n - rank(A)}  &\text{(Rank-nullity.)} \\
        =&\text{ n - rank(A) - rank(B) + rank(AB) + null(AB)} &\text{(Rank-nullity.)} \\
        \geq&\text{ n - rank(A) - rank(AB) + rank(AB) + null(AB)} &\text{(null(AB)} \geq \text{null(B).)}   \\
        =&\text{ n - rank(A) + null(AB)}    &\text{(Simplifying.)}  \\
        =&\text{ null(A) + null(AB)}    &\text{(Rank-nullity.)}    \\
        \geq&\text{ null(AB)}   &\text{(0} \leq \text{null(A).)} \\
        \end{align*}
        This proves null(AB) $\leq$ null(A) + null(B). $\square$
        \hop
        \item rank(A + B) $\leq$ rank(A) + rank(B).
        \jump

        This is a direct implication of the Dimension theorem, i.e. for vector space V and sub-spaces U, W of V, 
        \[
        \text{dim(U + W) = dim(U) + dim(W) - dim(U } \cap \text{ W).}
        \]
        Substituting U = im(A), W = im(B), and V = $\R^{n\times n}$,
        
        \[
        \text{dim(im(A)) + dim(im(B)) - dim(im(A } \cap \text{ B)) = dim(im(A+B)).}
        \]
        
        Since the dimension of a subspace of a vector space is positive, we can throw away the intersecting term to yield,
        \[
        \text{rank(A + B)} \leq \text{ rank(A) + rank(B).}
        \]
        $\square$
    \end{enumerate}

    \hop
    \item For A, B $\in$ $\R^{n \times n}$, show that if AB = 0, then rank(A) + rank(B) $\leq$ n.
    \jump

    Since AB = 0, for any vector \textbf{x} $\in$ $\R^n$, AB\textbf{x} = 0. Thus null(AB) = n, and rank(AB) = 0. From the previous part, we can use the second inequality to prove this one.

    \begin{align*}
        &\text{null(AB)} \leq \text{ null(A) + null(B)} &\text{(Given.)}    \\
        &\text{n} \leq \text{ null(A) + null(B)}    &\text{(null(AB) = n.)} \\
        &\text{n} \leq \text{n - rank(A) + n - rank(B)} &\text{(Rank-nullity.)} \\
        &\text{-n} \leq \text{-rank(A) - rank(B)}   &\text{(Simplifying.)} \\
        &\text{n} \geq \text{rank(A) + rank(B)} &\text{(Multiplying by -1.)}
    \end{align*}

    Thus the inequality is proven. $\square$
%end problem 1
\end{enumerate}

\item 
\begin{enumerate}
\item Let A $\in$ $\R^{m \times n}$ and B $\in$ $\R^{p \times q}$. Show that,
\[
\text{rank} \Bigg(
\begin{bmatrix}
A & 0 \\
0 & B
\end{bmatrix}
\Bigg) = \text{rank(A) + rank(B)}
\]
\jump

Let X be the provided block matrix, for simplicity. Among the first m rows of X, suppose we can find up $l$ linearly independent vectors. Repeating this for the last p rows, suppose we can find up to $r$ linearly independent vectors. Collect these row vectors into a subspace of X, noting that this subspace of X can represent all independent rows of A and B. Since this is a subspace of X, we can say rank(X) $\geq$ rank(A) + rank(B).

On the other hand, we can note that there X can can have up to a max of rank(A) + rank(B) linearly independent vectors, since if there were an additional linearly independent vector, it would have to note be in either A nor B, which is impossible. Thus rank(X) $\leq$ rank(A) + rank(B), which means they are equal. $\square$  
\hop
\item For \textbf{x} = $[x_1, x_2, ..., x_m]^{T}$ $\in$ $\R^{m}$ and \textbf{y} = $[y_1, y_2, ..., y_m]^{T}$ $\in$ $\R^{n}$, observe that \textbf{xy}$^{T}$ $\in$ $\R^{m \times n}$. Let A $\in$ $\R^{m \times n}$. Show that rank(A) = 1 iff A = \textbf{xy}$^{T}$ for some nonzero \textbf{x} $\in$ $\R^{m}$ and \textbf{y} $\in$ $\R^{m}$.
\jump

For the forward implication, we need to construct \textbf{x} and \textbf{y} from A, given rank(A) = 1. By this constriction, and by definition, we can only find 1 linearly independent column of A. Let \textbf{y} be this column, and without loss of generality, choose the first column as the linearly independent column. Denote \textbf{x} as the collection of multiples of \textbf{y} needed to represent each column of A (i.e., $A_{*,j} = x_j \textbf{y}$). We can note that \textbf{x} $\in$ $\R^{m}$, thus we can write A = \textbf{xy$^{T}$}. 
\hop

For the backward implication, suppose $\exists$ \textbf{x} $\in \R^{m}$, $y \in \R^n$, and A $\in \R^{m \times n}$ with A = \textbf{xy$^T$}. Expanding \textbf{xy$^T$},
\[
\begin{pmatrix}
x_1 \\
x_2 \\
... \\
x_m
\end{pmatrix}
\begin{pmatrix}
y_1 &y_2 &... &y_n   
\end{pmatrix}
=
\begin{pmatrix}
    x_1\textbf{y} &x_2\textbf{y} &... &x_m\textbf{y}
\end{pmatrix}
\]

This leads to A containing columns of multiples of \textbf{y}, thus one can find only one linearly independent column of A. Therefore rank(A) = 1. $\square$

%end problem 2
\end{enumerate}

\item Let A $\in \R^{m \times n}$.
\begin{enumerate}
    \item Show that
    \[
    \text{ker(A$^{T}$A) = ker(A) \hspace{5mm} and \hspace{5mm} im(A$^{T}$A) = im(A$^T$).}
    \]
    Give an example to show this is not true over a finite field (e.g. a field of two elements $\mathbb{F}_2$ = $\{$0, 1$\}$ with binary arithmetic). 

\begin{enumerate}
    \item \textbf{\underline{ker(A$^{T}$A) = ker(A)}}:
    \hop
    
    - Let \textbf{x} $\in$ ker(A). Thus A\textbf{x} = 0, implying A$^T$A\textbf{x} = 0. Therefore ker(A) $ \subseteq$ ker(A$^T$A).

    \hop
    - Next let \textbf{y} $\in$ ker(A$^T$A). so A$^T$A\textbf{y} = 0. By Associativity, A$^T$(A\textbf{y}) = 0, thus A\textbf{y} $\in$ ker(A$^T$). Note that the kernel of the transpose of A is equal to its orthogonal image, i.e. ker(A$^T$) = im(A)$^\perp$. Furthermore, since our vector is of the form A\textbf{z} for some \textbf{z} $\in \R^n$, A\textbf{y} $\in$ im(A). Thus A\textbf{y} $\in$ im(A)$^\perp$ $\cap$ im(A) = $\{0\}$.
    Then A\textbf{y} = 0, so \textbf{y} $\in$ ker(A), completing the equality. 

    \jump
    \item \textbf{\underline{im(A$^{T}$A) = im(A$^T$)}}:
    \hop
    
    - Let \textbf{y} $\in$ im(A$^T$A). Then $\exists$ \textbf{x} $\in \R^n$ such that y = A$^T$A\textbf{x}. By Associativity, \textbf{y} = A$^T$(A\textbf{x}). Since \textbf{y} is of the form A$^T$\textbf{z} for some \textbf{z} $\in \R^m$, \textbf{y} $\in$ im(A$^T$).
    
    \hop
    - Next let \textbf{z} $\in$ im(A$^T$). $\exists$ \textbf{y} $\in \R^m$ such that \textbf{z} = A$^T$\textbf{y}. Consider x $\in$ im(A). $\exists$ \textbf{w} $\in \R^n$ with \textbf{x} = A\textbf{w}. Choose \textbf{y} such that \textbf{y} = A\textbf{w} $\in$ im(A). Then \textbf{z} = A$^T$A\textbf{w}, implying \textbf{z} $\in$ im(A$^T$A) and completing the equality.  
\end{enumerate}

    Giving an example where ker(A$^T$A) is not equal to ker(A), take A to be
    \[
    \text{A = }
    \begin{bmatrix}
        0   &1  \\
        0   &0
    \end{bmatrix}
    ; \hspace{4mm}
    \text{A$^T$ = } 
    \begin{bmatrix}
        0   &0  \\
        1   &0
    \end{bmatrix}
    \]
Then, 
\[
\text{A$^T$A =}
\begin{bmatrix}
    0   &1  \\
    0   &0
\end{bmatrix}
\begin{bmatrix}
    0   &0  \\
    1   &0
\end{bmatrix}
\hspace{3mm} =
\hspace{3mm}
\begin{bmatrix}
    1   &0  \\
    0   &0
\end{bmatrix}
\]
Calculating the kernels of each,
\[
\text{ker(A) : }
\begin{bmatrix}
    0   &1  \\
    0   &0
\end{bmatrix}
\begin{pmatrix}
    x   \\
    y
\end{pmatrix}
\hspace{3mm}
=
\hspace{3mm}
\begin{pmatrix}
    y   \\
    0
\end{pmatrix}
\hspace{3mm}
\implies
\hspace{3mm}
\text{ker(A) = $\{$(x, 0) : x $\in \R$ $\}$}
\]
\[
\text{ker(A$^T$A) : }
\begin{bmatrix}
    1   &0  \\
    0   &0  \\
\end{bmatrix}
\begin{pmatrix}
    x   \\
    y
\end{pmatrix}
\hspace{3mm}
=
\hspace{3mm}
\begin{pmatrix}
    x    \\
    0
\end{pmatrix}
\hspace{3mm}
\implies
\hspace{3mm}
\text{ker(A$^T$A) = $\{$(0, y) : y $\in \R$ $\}$}
\]

Therefore, ker(A) $\neq$ ker(A$^T$A).

\jump
\item Show that
\[
\text{A$^T$A}\textbf{x} = \text{A$^T$}\textbf{b}
\]
always has a solution (even if A\textbf{x} = \textbf{b} has no solution). Give an example to show that this is not true over a finite field.  

\hop
Since im(A$^T$A) = im(A$^T$), any vector in the image of im(A$^T$A) is in the image of im(A$^T$). so if A$^T$\textbf{b} $\in$ im(A$^T$), then A$^T$\textbf{b} $\in$ im(A$^T$A), thus $\exists$ \textbf{x} $\in \R^{n}$ such that A$^T$A\textbf{x} = A$^T$\textbf{b}. Thus even if A\textbf{x} = \textbf{b} has no solution, A$^T$A\textbf{x} = A$^T$\textbf{b} will have a solution. 

Over a finite field, we can choose 
\[
A = 
\begin{pmatrix}
    1 &0    \\
    0 &0
\end{pmatrix}
,
\hspace{5mm}
\textbf{b} = 
\begin{pmatrix}
    0   \\
    1
\end{pmatrix}.
\]

\hop
Note this system has no solutions. Then, $A^TA = A^T = A$, keeping our original system with no solutions.
% end problem 3
\end{enumerate}
\jump
\item Let \textbf{v}$_1$, ..., \textbf{v}$_n \in \R^n$. Let r $\leq$ n and G$_r$ = [$g_{ij}$] $\in \R^{r \times r}$ be the matrix with
\[
g_{ij} = \textbf{v}_{i}^{T}\textbf{v}_{j}
\]
for $i, j$ = 1,...,r. This is called a Gram matrix. 
\begin{enumerate}
    \item Show that \textbf{v}$_1$, ..., \textbf{v}$_n$ are linearly independent iff null(G$_r$) = 0. 
    
    \hop
    As this is an if and only if statement, we need to prove both the forward and backward directions. 
    
    For the forward direction, define matrix M such that its rows contain the corresponding transposed vector, i.e., 
    \[
    \text{M}_{i,*} = \textbf{v}_i^T.
    \]
    Where i = 1, ..., r. Note that M\textbf{v}$_1$ represents the first column of G$_r$. The statement null(G$_r$) = 0 is equivalent to saying rank(G$_r$) = r, or that the columns of G$_r$ are linearly independent. Thus, All we need to do is show that M\textbf{v}$_1$, ..., M\textbf{v}$_r$ are linearly independent. This statement is fortunately equivalent to showing the rank(M) = r, which since {v}$_1$, ..., {v}$_r$ are given as linearly independent, is guaranteed.
    Thus the forward direction has been proven.

    \hop
    The backward direction is quite simple to prove. Since null(G$_r$) = 0, this means that rank(G$_r$) = r, r $\leq$ n. This is equivalent to saying its columns are linearly independent, or M\textbf{v}$_1$, ..., M\textbf{v}$_n$ are linearly independent. We just now need to show that \textbf{v}$_1$, ..., \textbf{v}$_n$ are linearly independent. Take c$_1$, ..., c$_n \in \R$. We just need to show that c$_1$\textbf{v}$_1$ + ... + c$_n$\textbf{v}$_n$ = 0 iff c$_1$ = ... = c$_n$ = 0. Applying M to both sides and invoking the linearity of M gives
    \[
    \text{c$_1$M}\textbf{v}_1 + ... + \text{c$_n$M}\textbf{v}_n \text{= M0 = 0}
    \]
    Since M\textbf{v}$_1$, ..., M\textbf{v}$_n$ are linearly independent, this equation is zero iff c$_1$ = ... = c$_n$ = 0, thus proving \textbf{v}$_1$, ..., \textbf{v}$_n$ are linearly independent. This means the backward direction has been proven, and equality holds. $\square$

    \hop
    \item Show that G$_r$ = $\mathbb{I}_r$ iff \textbf{v}$_1$, ..., \textbf{v}$_r$ are pairwise orthogonal unit vectors, i.e., $\lVert$\textbf{v}$_i$$\rVert_2$ = 1 for all $i$ = 1, ..., r and \textbf{v}$^T_i$\textbf{v}$_j$ = 0 for all $i \neq j$. If this holds, show that
\begin{align}
    \sum_{i = 1}^r(\textbf{v}^T\textbf{v}_i)^2 \leq \lVert \textbf{v} \rVert_2^2 \tag{4.1} 
\end{align}
for all \textbf{v} $\in \R^n$. What can you say about \textbf{v}$_1$, ..., \textbf{v}$_r$ if equality always holds in (4.1) for all \textbf{v} $\in \R^n$? 

\jump
For the forward implication we have that G$_r$ = $\mathbb{I}_r$. Note that we can express the identity matrix as $\delta_{ij}$, where $\delta_{ij}$ is the Kronecker-delta symbol, for i, j = 1, ..., r. Thus g$_{ij}$ = \textbf{v}$_i^T$\textbf{v}$_j$ = $\delta_{ij}$ for i, j = 1, ..., r. Note additionally that \textbf{v}$_i^T$\textbf{v}$_j$ is simply the inner product of the two vectors \textbf{v}$_i$, \textbf{v}$_j$. So for i = j, g$_{ii}$ = $\delta_{ii}$ = 1 =  $\braket{v_i | v_i} = \lVert \textbf{v}_i \rVert_2^2$. So each vector has length 1. for i $\neq$ j, g$_{ij}$ = $\delta_{ij}$ = 0 =  $\braket{v_i | v_j}$. Thus \textbf{v}$_1$, ..., \textbf{v}$_r$ are orthogonal with unit length, implying G$_r$ = $\mathbb{I}_r$.

\hop
For the backward implication we have \textbf{v}$_1$, ..., \textbf{v}$_r$ are pairwise orthogonal unit vectors. We then want to show G$_r$ = $\mathbb{I}_r$. Note that g$_{ij}$ = \textbf{v}$_i^T$\textbf{v}$_j$ = $\braket{v_i | v_j}$. Since \textbf{v}$_1$, ..., \textbf{v}$_r$ are pairwise orthogonal, $\braket{v_i | v_j}$ = $\lVert \textbf{v}_i \rVert _2^2\delta_{ij}$, and since \textbf{v}$_1$, ..., \textbf{v}$_r$ are unit vectors, $\lVert \textbf{v}_i \rVert _2^2$ = 1 for all $i$ = 1, ..., r. Therefore $g_{ij} = \braket{\textbf{v}_i | \textbf{v}_j} = \delta_{ij} = \mathbb{I}_r$ for i, j = 1, ..., r.
This proves the backward implication, proving the statement. $\square$

\hop
To prove (4.1), we can expand out $\lVert \textbf{v} \rVert ^2_2$ and compare summations.

\[
\lVert \textbf{v} \rVert ^2_2 = \sum_{i = 1}^n \textbf{v}_i^2 = \sum_{i= 1}^n (e^t_i\textbf{v})^2
\]

Here we're just taking the inner product of the vector \textbf{v} with the standard basis vector $e_i$, $i = 1,..., n$. Since \textbf{v}$_1$, ..., \textbf{v}$_r$ are assumed to be pairwise orthogonal unit vectors, these \textbf{v}$_i$'s can be swapped with $r$ standard unit vectors (or there exists some mapping from the \textbf{v}$_i$'s to the standard unit vectors).

\[
\hspace{-5mm}\implies \sum_{i = 1}^r(\textbf{v}^tv_i)^2 = \sum_{i = 1}^r (\textbf{v}^te_i)
\]

Since $r \leq n$, the summation running to $r$ will always be less than or equal to the summation running to $n$, with equality holding only when $r = n$.
% End problem 4
\end{enumerate}

\item Let A, B $\in \R^{m\times n}$. For any $i, j \in \{1, ..., n\}$, let $E_{ij} \in \R^{p \times q}$ denote the matrix with one in the ($i,j$)-th entry and zeros everywhere else.
\begin{enumerate}
    \item Describe the matrix $E_{ij}AE_{kl} \in \R^{m \times n}$ where $E_{ij} \in \R^{p\times m}$ and $E_{kl} \in R^{n \times q}.$ 

    \jump
    Although having the same end matrix, which E-matrix you choose to apply to A first will result in a different intermediate matrix. Via associativity, I will break this down into two cases.
\begin{enumerate}
    \item \underline{\textbf{Case 1:} $(E_{ij}$A)$E_{kl}$}

    \hop
    This results in an intermediate matrix of size p by n, where the j-th row of A is extracted and emplaced as the i-th row of the resulting matrix. I will provide an example. Say, A is 2 by 3, and i = 1, j = 2. The resulting intermediate product is then,
    \[
    E_{12}A
    \hspace{3mm}
    =
    \hspace{3mm}
    \begin{pmatrix}
        0   &1    \\
        0   &0    
    \end{pmatrix}
    \begin{pmatrix}
        1   &2  &3  \\
        4   &5  &6
    \end{pmatrix}
    \hspace{3mm}
    =
    \hspace{3mm}
    \begin{pmatrix}
        4   &5  &6  \\
        0   &0  &0
    \end{pmatrix}
    \]
    
    \hop
    \item \underline{\textbf{Case 2:} $E_{ij}$(A$E_{kl})$}
    
    \hop
    This results in an intermediate matrix of size m by q, where the k-th column of A is extracted and emplaced as the l-th column of the resulting matrix. I'll keep with the example given in the previous case, where k = 1 and l = 3.
    \[
    AE_{13}
    \hspace{3mm}
    =
    \hspace{3mm}
    \begin{pmatrix}
        1   &2  &3  \\
        4   &5  &6
    \end{pmatrix}
    \begin{pmatrix}
        0   &0  &1  \\
        0   &0  &0  \\
        0   &0  &0
    \end{pmatrix}
    \hspace{3mm}
    =
    \hspace{3mm}
    \begin{pmatrix}
        0   &0  &1  \\
        0   &0  &4
    \end{pmatrix}
    \]

    \hop
    Where the final matrix is 
    \[
    E_{12}AE_{13}
    \hspace{3mm}
    =
    \hspace{3mm}
    \begin{pmatrix}
        0   &1  \\
        0   &0
    \end{pmatrix}
    \begin{pmatrix}
        0   &0  &1  \\
        0   &0  &4
    \end{pmatrix}
    \hspace{3mm}
    =
    \hspace{3mm}
    \begin{pmatrix}
        0   &0  &4  \\
        0   &0  &0
    \end{pmatrix}
    \]

    \hop
    As you can see, Applying $E_{12}$ to our intermediate matrix places the second row into the first row of the final matrix. Note the final matrix is of size p by q.
\end{enumerate}

\hop
Therefore, $E_{ij}$ extracts the j-th row of A and places it into the i-th row, and $E_{kl}$ places the k-th column into the l-th column. As shown above, this results in different intermediate matrices, but the final matrix is guaranteed to be the same due to Associativity.

\jump
\item Find expressions tr($A^TB$), tr($AB^T$), tr($BA^T$), tr($B^TA$) in terms of entries of A and B. What if A = B? What if B = $E_{ij} \in \R^{m \times n}$?

\hop
Note that for any matrix C $\in \R^{m \times n}$, tr(C) = tr(C$^T$). This is due to the diagonal of C not being disrupted when transposing. Due to this, it suffices to find expressions tr($A^TB$) and tr($AB^T$).

\hop
Taking the standard formula for matrix multiplication,
\[(AB)_{ij} = \sum_{k = 1}^m a_{ik}b_{kj}\]

setting j = i and summing over i will give us a formula for the trace of the product of two matrices.
\[
\hspace{-5mm}\implies \text{tr}(AB) = \sum_{i = 1}^m \sum_{k = 1}^m a_{ik}b_{ki}
\]

so all we need to do is swap indices to suit our input matrices.
\[
\hspace{2mm} \text{tr}(A^TB) = \sum_{i = 1}^m\sum_{k = 1}^m a_{ki}b_{ki}
\]
\[
\hspace{2mm} \text{tr}(AB^T) = \sum_{i = 1}^m\sum_{k = 1}^m a_{ik}b_{ik}
\]
So if A = B, we can write
\[
\hspace{-3mm} \text{tr}(A^TA) = \sum_{i = 1}^m\sum_{k = 1}^m a_{ki}^2
\]
\[
\hspace{-3mm} \text{tr}(AA^T) = \sum_{i = 1}^m\sum_{k = 1}^m a_{ik}^2
\]

\hop
Note that by this calculation, tr($AB^T$) = tr($A^TB$), up to a relabelling of indices. If B = $E_{ij} \in \R^{m \times n}$, it breaks the formula I've given, since B did not depend on indices. Instead, it will have to be evaluated separately. Reminder that E on the right of A swaps columns, E on the left swaps rows. 

\jump
For the case of tr($A^TB$), the resulting matrix will have one non-zero column, where the i-th column of A is represented as the j-th column. Since A is being transposed, the i-th \textit{row} is being represented as the j-th column. This results in the element in the i-th row, j-th column being the trace. So tr($AE_{ij}$) = $a_{ij}$.

\jump
For the case of tr($AB^T$), the resulting matrix will have one non-zero column, where the j-th column of A is represented as the i-th column. These are swapped since E is transposed ($E_{ij} \rightarrow E_{ji}$). This means the element on the diagonal is $a_{ii}$, therefore tr($AE_{ji}$) = $a_{ii}$. 

\jump
\item Suppose m = n and tr(A) = tr(A$^2$) = ... = tr(A$^n$) = 0. Show that A$^n$ = 0.

\jump
Let $\lm_1, ..., \lm_n$ be the eigenvalues of A, with associated eigenvector \textbf{v}. Note that $A^2\textbf{v} = A(A\textbf{v}) = A\lm\textbf{v} = \lm^2\textbf{v}$, thus \textbf{v} is an eigenvector of $A^k$ with eigenvalue $\lm^k$. We are given tr(A) = tr(A$^2$) = ... = tr(A$^n$) = 0. Since the trace of a matrix is the sum of its eigenvalues, we can write the following system of equations:

\begin{align*}
    &\lm_1 + \ ... \ + \lm_n = 0  \\
    &\lm_1^2 + \ ... \ + \lm_n^2 = 0 \\
    &... \\
    &\lm_n^n + \ ... \ + \lm_n^n = 0
\end{align*}

This system of equations is called the symmetric powers of A. Since these are all equal 0, this will help tremendously in the next step. 

\hop
Writing out the characteristic polynomial of A, we get,

\[
\chi_A(x) = (x - \lm_1)(x-\lm_2)(...)(x-\lm_n)
\]

which, when expanded, is of the form,

\[
x^n - \bigg(\sum_i\lm_i\bigg)x^{n-1} \bigg(\sum_{i<j\leq n} \lm_i\lm_j\bigg)x^{n - 2} + ... + (-1)^ne_n(\lm_1, ..., \lm_n),
\]

Where $e_i$ is an Elementary symmetric polynomial, defined as the sum of all distinct products of n distinct variables. Note that a property of elementary symmetric polynomials is they can be expressed as sums and products of the symmetric powers stated above. Since the symmetric powers are all equivalent to zero, this means each Elementary symmetric polynomial is a sum and product of zero, thus all coefficients of our characteristic polynomial equal zero. This leaves us with,

\[
\chi_A(x) = x^n.
\]

By The Cayley-Hamilton theorem, the matrix A satisfies its characteristic polynomial, that is $\chi_A(A) = 0$. Thus $A^n = 0$, which proves the statement. $\square$
\hop
\item Suppose $m = n$. Show that
    \begin{enumerate}
        \item tr$[(AB)^{k}]$ = tr$[(BA)^k]$.

        \hop
        This can be done via induction on k.

        \hop \underline{Base Case: k = 1}

        \hop
        Then,
        \[
        (AB)_{ii} = \sum_{k = 1}^n a_{ik}b_{ki},
        \hspace{5mm}
        (BA)_{ii} = \sum_{k = 1}^n b_{ik}a_{ki}
        \]
        
        \hop
        These summations are equivalent, up to relabelling of indices. Thus,

        \hop
        \[
        \text{tr}[(AB)] = \sum_{i = 1}^n (AB)_{ii} = \sum_{k = 1}^n a_{ki}b_{ik},
        \hspace{5mm}
        \text{tr}[(BA)] = \sum_{i = 1}^n (BA)_{ii} = \sum_{k = 1}^n b_{ik}a_{ki}
        \]

        \hop
        \[
        \hspace{-10mm}\implies \text{tr}[(AB)] = \text{tr}[(BA)]
        \]

        \hop
        Thus the base case has been proven. Next, assume the claim holds for some $l < k$, we will show the $l+1$ case holds. I.e., we wish to show

        \hop
        \[
        \text{tr}\big[ (AB)^{l+1}\big] = \text{tr}\big[ (BA)^{l+1}\big]
        \]

        \begin{align*}
            \text{tr}\big[ (AB)^{l+1}\big] &= \text{tr}\big[ (AB)^{l}(AB)\big]  &\text{(Matrix Exponent.)} \\
            &= \text{tr}\big[ (BA)^{l}(AB)\big] &\text{(Inductive Hypothesis.)} \\
            &= \text{tr}\big[ (BA)^{l}(AB)^T\big] &\text{(From Problem 5, part b.)}\\
            &= \text{tr}\big[ (BA)^{l}(B^TA^T)\big] &\text{(Matrix Transpose Property.)}\\
            &= \text{tr}\big[ (BA)^{l}(BA)\big] &\text{(Problem 5, part b applied  to A and B.)}\\
            &= \text{tr}\big[ (BA)^{l+1}\big] &\text{(Matrix Exponent.)}
        \end{align*}

        Therefore, the statement holds by induction. $\square$

        \jump
        \item and if AB = 0, Then tr$[(A+B)^k]$ = tr$(A^k)$ + tr$(B^k)$, for all $k \in \mathbb{N}$

        \jump
        Since in the previous part we showed that matrices commute under the trace operator, we are free to use binomial expansion on $(A+B)^k$.

        \[
        \implies \text{tr}\big[ (A+B)^k\big] = \text{tr}\big[ \sum_{j = 0}^k \binom{k}{j} A^{k - j}B^j\big].
        \]

        From this expansion, we can take out the two cases where $j = 0$ and $j = k$ to get our leading terms.

        \[
        \hspace{15mm}\text{tr}\big[ \sum_{j = 0}^k \binom{k}{j} A^{k - j}B^j\big] = \text{tr}\big[ A^k + B^k + \sum_{j = 1}^k \binom{k}{j} A^{k - j}B^j\big].
        \]

        We can note that under the summation, each term will have an AB product in it, thus making the entire term 0. We can discard the summation in that case. 

        \[
        \text{tr}\big[ (A+B)^k\big] = \text{tr}\big[A^k + B^k\big].  
        \]

        And finally, since the trace function is a linear operator, we can break this apart into two pieces.

        \[
        \hspace{-5mm}\implies \text{tr}\big[ (A+B)^k\big] = \text{tr}(A^k) + \text{tr}(B^k), 
        \]

        thus proving the statement. $\square$
        
    \end{enumerate}

% End number 5
\end{enumerate}

\jump
\item Let A $\in \C^{n\times x}$. Recall that A is diagonalizable iff there exists an invertible X $\in \C^{n \times n}$ such that $X^{-1}AX = \Lambda$, a diagonal matrix.
\begin{enumerate}
    \item Show that A is diagonalizable if and only if its minimal polynomial is of the form 
    \[
    m_A(x) = (x - \lm_1)\cdots(x - \lm_d)
    \]
    where $\lm_1$, ..., $\lm_d \in \C$ are all distinct. Hence deduce for a diagonalizable matrix, the degree of its polynomial equals the number of distinct eigenvalues.

\jump
Since any square matrix can be written in Jordan Canonical Form, i.e. 
\[
A = XJX^{-1}.
\]

Where $J$ is composed of block matrices along its diagonal of the form $J_k = \lm_kI + N$, where N is a nilpotent matrix of nilpotency p and $\lm_k$ is the k-th eigenvalue of A. There are a few things to note about the Jordan Canonical form of a matrix:
\begin{enumerate}
    \item The number of total blocks inside $J$ is equal to the number of distinct eigenvectors, thus each Jordan block has one eigenvector. 
    \item For the minimum polynomial $\mu_A$ of A, we need only investigate $J$ itself. That is, $\mu_A(A) = X\mu_A(J)X^-1$.
    \item The size of a Jordan block $J_k$ depends on how many eigenvectors are associated with the eigenvalue $\lm_k$. That is, the eigenspace of $\lm_k$ has size equal to the size of $J_k$.  
\end{enumerate}

For the backward implication, we suppose the minimal polynomial is of the form 
\[
\mu_A(x) = (x - \lm_1)(...)(x - \lm_d)
\]

where $\lm_1, ..., \lm_d$ are the only distinct eigenvalues of A. By the Cayley-Hamilton Theorem, $\mu_A(A) = 0$, i.e.

\[
\mu_A(A) = (A - \lm_1I)(A-\lm_2I)(...)(A - \lm_dI) = 0
\]

since $\mu_A(A) = 0$, then A must zero out all non-diagonal terms of $J$, leaving $J$ with entries only along the diagonal. Thus A is diagonalizable. 

\hop
For the forward implication, we assume A is diagonalizable. It's Jordan Canonical form is thus a diagonal matrix. Its characteristic polynomial is thus
\[
\chi_A(x) = (x - \lm_1)^{\alpha_1}(...)(x - \lm_d)^{\alpha_d}
\]

where $\alpha_1,..., \alpha_d > 1$ are the algebraic multiplicities of the eigenvalue $\lm_i$. 

\jump
\underline{Claim:} $\mu_A(x) = (x -\lm_1)(...)(x - \lm_d)$, i.e. $\alpha_1 = ... = \alpha_i = 1$.

\jump
Suppose false, meaning $\exists$ $g < d$ such that $\mu_A(x) = (x -\lm_1)(...)(x - \lm_g)$. Consider the Jordan block associated with the i-th eigenvalue, $J_i$ with $g < i \leq d$. Since $\lm_1,..., \lm_d$ are distinct eigenvalues, $\lm_k$ cannot zero out the diagonal values of the Jordan block $J_i$ for $k \leq g$, thus making this impossible. Therefore, $\mu_A(x) = (x -\lm_1)(...)(x - \lm_d)$. Now we just need to show $\alpha_1 = ... = \alpha_i = 1$. This is simple, since 
\[
\chi_A(J) = (J_1 - \lm_1I)^{\alpha_1}(...)(J_d - \lm_dI)^{\alpha_d} = 0.
\]

Since $J_k = \lm_k + N$, $J_k - \lm_kI = N$. And since A is diagonalizable, $N = 0$, meaning A has no off-diagonal terms in its Jordan form. Thus  $\alpha_1 = ... = \alpha_i = 1$, finishing the proof. $\square$

\jump
\item Let $A$ be diagonalizable. Let $\textbf{x}_1, ..., \textbf{x}_n \in \C^n$ be $n$ linearly independent right eigenvectors of A, and $\textbf{y}_1, ..., \textbf{y}_n \in \C^n$ be $n$ linearly independent left eigenvectors of A. Show that we may choose $\textbf{x}_1, ..., \textbf{x}_n$ and $\textbf{y}_1, ..., \textbf{y}_n$ so that any vector $\textbf{v} \in C^n$ can be expressed as 
\[
\textbf{v} = \sum_{i = 1}^n(\textbf{y}^T_i\textbf{v})\textbf{x}_i.
\]

If we write $X$ = $[\textbf{x}_1, ..., \textbf{x}_n] \in \C^{n\times n}$ and $Y$ = $[\textbf{y}_1, ..., \textbf{y}_n] \in \C^{n\times n}$, what is the relation between $X$ and $Y$?

By some manipulation of the summation, 
\begin{align*}
    \textbf{v} &= \sum_{i = 1}^n(\textbf{y}^T_i\textbf{v})\textbf{x}_i &\text{(Given.)} \\
    &= \sum_{i = 1}^n\textbf{x}_i(\textbf{y}^T_i\textbf{v}) &\text{(Vector-scalar commutation.)}\\
    &= \sum_{i = 1}^n\textbf{x}_i\textbf{y}^T_i\textbf{v} &\text{(Associativity.)}\\
    &= \sum_{i = 1}^n(\textbf{x}_i\textbf{y}^T_i)\textbf{v} &\text{(Associativity.)}\\
    &= \bigg[\sum_{i = 1}^n(\textbf{x}_i\textbf{y}^T_i)\bigg]\textbf{v} &\text{(Distributive Law.)}\\
\end{align*}

Thus we can note that 
\[
\sum_{i = 1}^n(\textbf{x}_i\textbf{y}^T_i) = I.
\]

This should hold for any choice of linearly independent eigenvectors, provided we are in the basis of the eigenvectors of A. If we were to write $X$ = $[\textbf{x}_1, ..., \textbf{x}_n] \in \C^{n\times n}$ and $Y$ = $[\textbf{y}_1, ..., \textbf{y}_n] \in \C^{n\times n}$, then we can interpret the above summation as multiplying the i-th column of X with the i-th row of Y and summing over all results. This is one of the ways to interpret Matrix Multiplication, therefore: $XY = I$. Since $X$ and $Y$ are written to have linearly independent columns, both are invertible, thus $X = Y^{-1}$ and vice versa.
% End number 6 
\end{enumerate}

% End numbering 
\end{enumerate}
\end{document}