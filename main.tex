\input{definitions}

% Enter the specific assignment number and topic of that assignment below, and replace "Your Name" with your actual name.
\title{STAT 30900: Homework 2}
\author{Caleb Derrickson}
\date{October 31, 2023}

\begin{document}
\onehalfspacing
\maketitle

{\color{cit}\vspace{2mm}\noindent\textbf{Collaborators:}} The TA's of the class, as well as Kevin Hefner, and Alexander Cram.

\tableofcontents

\newpage
\section{Problem 1}
The files required for this problem can be found in the subfolder hw2 in Canvas. The matrix in processed.mat (Matlab format) or processed.txt (comma separated, plain text) is a 49 × 7 matrix where each row is indexed by a country in row.txt and each column is indexed by a demographic variable in column.txt, ordered as in the respective files. So for example, if we denote the matrix by

\[
A 
= 
\begin{bmatrix}\textbf{a}_1^T \\ \textbf{a}_2^T \\ \vdots \\ \textbf{a}_{49}^T\end{bmatrix}
=
\begin{bmatrix} \textbf{$\alpha$}_1,&\dots,&\textbf{$\alpha$}_7\end{bmatrix}
=
\begin{bmatrix}
    a_{11} &a_{12} &\dots &a_{17} \\
    a_{21} &a_{22} &\dots &a_{27} \\
    \vdots &\vdots &\ddots &\vdots \\
    a_{49,1} &a_{49,2} &\dots &a_{49,7}
\end{bmatrix}
\in \R^{49\times 7},
\]
then $a_{23}$ = -0.2743 is Austria’s population per square kilometers (row index 2 = Austria, column index 3 = population per square kilometers). As you probably notice, this matrix has been slightly preprocessed. If you want to see the raw data, you can find them in raw.txt (e.g. the actual value for Austria’s population per square kilometers is 84) but you don’t need the raw data for this problem.

\subsection{Problem 1, part a}
Show that to plot the projections of the row vectors (i.e., samples) $\textbf{a}_1, ..., \textbf{a}_{49} \in \R^{7}$ onto the two-dimensional subspace span $\{ \textbf{v}_j, \textbf{v}_k \cong \R^2\}$, we may simply plot the $n$ points 
\[
    \{(\sigma_ju_{ij}, \sigma_ku_{ik}) \in \R^2 : i = 1, ..., 49\}
\]
where $U = [u_{ij}] \in \R^{49\times 49}$ and $\Sigma = $ diag$(\sigma_1, ..., \sigma_p) \in \R^{49 \times 7}$ are the matrix of left singular vectors and matrix of singular values respectively.  

\partbreak
\begin{solution}

     Denote the Singular Value Decomposition of $A$ as $U\si V^t$, where $U, V$ are matrices constructed by orthonormal vectors. They are related to each other by $A\textbf{v}_k = \sigma_k\textbf{u}_k$, where $\textbf{u}_k, \textbf{v}_k$ is the $k-$th column vector of $U$ and $V$, respectively, and $\sigma_k$ is the $k$-th singular value of $A$. Similarly, $A^*\textbf{u} = \sigma_k\textbf{v}_k$. Thus, if there were some other $\textbf{u}_i$, such that $A^*\textbf{u}_i = \sigma_i\textbf{v}_k$, then 
     \[
     \textbf{v}_k^*\textbf{v}_k = (\frac{1}{\sigma_i}A^*\textbf{u}_i)^*A^*\textbf{u}_k = \frac{1}{\sigma_i}\textbf{u}_i^*AA^*\textbf{u}_k = \frac{\sigma_k^2}{\sigma_i}\textbf{u}_i^*\textbf{u}_k = 0
     \]

      since the set of \textbf{u} vectors are orthonormal Thus, no other $\textbf{u}$ contributes to $\textbf{v}_k$, meaning if we want to project onto the given subspace, we only need to plot $\{(\sigma_ju_{ij}, \sigma_ku_{ik}) \in \R^2 : i = 1, ..., 49\}$.
\end{solution}

\newpage
\subsection{Problem 1, part b}
Find the first two right singular vectors of $A, \textbf{v}_1, \textbf{v}_2 \in \R^7$ . Project the data onto the two-dimensional space span$\{\textbf{v}_1, \textbf{v}_2\} \cong \R^2$ . Plot this in a graph where the x- and y-axes correspond to $\textbf{v}_1$ and $\textbf{v}_2$ respectively and where the points correspond to the countries — label each point by the country it corresponds to. Identify the two obvious outliers.
\partbreak
\begin{solution}

    I will provide the plot below, followed by the code. It seems the two countries I found as outliers are Singapore and Hong Kong.

    \begin{figure}[h]
        \centering
        \includegraphics[width = 0.5\textwidth]{Images/problem 1b plot.png}
        \label{fig:problem 1b}
    \end{figure}

\begin{lstlisting}    
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#filepaths for country data, names
filepathmat = 'datafiles/processed.txt'
filepathcont = 'datafiles/row.txt'
filepathdemvar = 'datafiles/column.txt'

#read in country names, then data with index as names
cont = pd.read_csv(filepathcont, header=None)
demvar = pd.read_csv(filepathdemvar, header=None)
df = pd.read_csv(filepathmat, header=None)
df = df.set_axis(cont[0], axis='index')

#Write to numpy array to do SVD
df_arr = df.to_numpy()
U, S, V = np.linalg.svd(df_arr)

testpts = []

j, k = 1, 2
for pts in range(U.shape[0]):
    testpts.append((S[j] *U[pts][j], S[k]*U[pts][k]))
pts = np.array(testpts)

#Finding outliers
outlier1 = np.argmin(pts[:, 0])
outpt1 = (outlier1, pts[outlier1, :])
outlier2 = np.argmin(np.delete(pts[:, 0], outlier1))
outpt2 = (outlier2, pts[outlier2, :])

outliers = np.array([df.index[outlier1], df.index[outlier2]])

for x, y in pts:
    plt.scatter(x, y)
    
plt.title("Country data")
plt.style.use('fivethirtyeight')
plt.gray
plt.ylabel(f'Projection {j}')
plt.xlabel(f'Projection {k}')
#Labelling outliers on plot
plt.annotate(f'{outliers[0]}', xy=(outpt1[1]) )
plt.annotate(f'{outliers[1]}', xy=(outpt2[1]) )
\end{lstlisting}
\end{solution}


\newpage
\subsection{Problem 1, part c}
Now do the same with the two left singular vectors of $A$, $\textbf{u}_1, \textbf{u}_2 \in \R^{49}$. Project the column vectors (i.e., variables) $\alpha_1, ..., \alpha_7 \in \R^{49}$ onto the two-dimensional space span$\{\textbf{u}_1, \textbf{u}_2\} \cong \R^2$ and plot this in a graph as before. Note that in this case, the points correspond to the demographic variables — label them accordingly.
\partbreak
\begin{solution}

The plot will be provided below, as well as the code. 

\begin{figure}[h]
    \centering
    \includegraphics[width = 0.5\textwidth]{Images/problem 1c plot.png}
    \label{fig:problem1c}
\end{figure}

\begin{lstlisting}
#Doing the same except for demographic variables
testpts = []

j, k = 1, 2
for pts in range(V.shape[0]):
    testpts.append((S[j] *V[pts][j], S[k]*V[pts][k]))
pts = np.array(testpts)


# Create the scatter plot
plt.scatter(pts[:, 0], pts[:, 1], c = [random.randint(0, x*10) for x in range(len(V))])

#Finding outliers
for i, (x, y) in enumerate(pts):
    plt.annotate(f'{demvar[0][i]}', xy=(x, y), fontsize=10)

plt.title("Country data")
plt.style.use('fivethirtyeight')
plt.gray
plt.ylabel(f'{df.index[k]}')
plt.xlabel(f'{df.index[j]}')
\end{lstlisting}
\end{solution}

\newpage
\section{Problem 2}
Let $A, B \in \R^{m \times n}$ where $A$ has full column rank.

\subsection{Problem 2, part a}
Show that 
\[
\min_{X \in \R^{n\times m}} \norm{AX - \id_m}_F
\]
has a unique solution. What is the minimum length solution, i.e., where $\norm{X}_F$ is minimum?
\partbreak
\begin{solution}

    Suppose false, that is, there exists $X, X' \in \R^{n \times m}$ with $X \neq X'$ that achieve minimum. Then, via SVD, we can deduce a contradiction. Note the norm we are working with is the Frobenius norm, i will omit this in writing. We can also write $\id_m = UU^T$.

{\small
    \alignbreak
    \begin{align}
        \norm{AX' - \id_m} &= \norm{AX - \id_m} &\text{(Both in argmin.)}\nonumber\\
        \norm{U\si V^t X' - UU^T} &= \norm{U\si V^t X - UU^T} &\text{(SVD of $A$.)}\nonumber\\
        \norm{\si V^TX' - U^T} &= \norm{\si V^TX - U^T} &\text{(Unitary invariance.)}\nonumber\\
        \tr \Big[ (\si V^TX' - U^T)^T(\si V^TX' - U^T)\Big] &= \tr \Big[ (\si V^TX' - U^T)^T(\si V^TX' - U^T)\Big] &\text{(Frobenius trace.)}\nonumber\\
        \tr \Big[ (\si V^TX')^T\si V^TX' - (\si V^TX'&)^TU^T - U\si V^TX' + UU^T\Big] =\\
        \tr \Big[ (\si V^TX)^T\si V^TX - (\si V^TX'&)^TU^T - U\si V^TX + UU^T\Big]\\
        1): \tr \Big[ X'^TV\si \si V^TX' - X'^TV\si U^T& - U\si V^TX' + UU^T\Big] &\text{(By symmetry 1 = 2.)}\nonumber\\
        \tr \Big[ X'^TV\si \si V^TX'\Big] - \tr \Big[ X'^TV\si U^T& \Big]- \tr \Big[ U\si V^TX'\Big] + \tr \Big[UU^T\Big] &\text{(Trace is linear.)}\nonumber\\
        \tr \Big[ X'^TV\si \si V^TX'\Big] - \tr \Big[ X'^TA^\dag& \Big]- \tr \Big[ AX'\Big] + \tr \Big[UU^T\Big] &\text{($A$ and $A\dag$.)}\nonumber\\
        \tr \Big[ V\si \si V^TX'X'^T\Big] - \tr \Big[ X'^TA^\dag& \Big]- \tr \Big[ AX'\Big] + \tr \Big[UU^T\Big] &\text{(Permutation in trace.)}\nonumber\\
        \tr \Big[ V\si \si V^T(X'X'^T - XX^T)\Big] + \tr \Big[& (X^T - X'^T)A^\dag \Big] + \tr \Big[ A(X - X')\Big] = 0 &\text{(Combining both.)}\nonumber\\
        \norm{V\si \si V^T(X'X'^T - XX^T)} + &\norm{(X^T - X'^T)A} + \norm{A(X - X')} = 0 &\text{(Frobenius definition.)}\nonumber\\
        \implies \norm{A(X - X')} = 0 &&\text{(All norms $\geq 0$, so all are 0.)}\nonumber\\
        \iff X - X' = 0 &&\text{(Norm definition.)}\nonumber\\
        \iff X = X'\nonumber
    \end{align}
    \alignbreak

}%
    Thus we found $X = X'$, which is a contradiction, so $X$ is unique. Aplologies for the bad formatting, this is the only way I could get it to fit.
\end{solution}

\newpage
\subsection{Problem 2, part b}
Show that the following method produces a symmetric matrix $X \in \R^{n \times n}$ that solves 
\[
\min_{X^T = X} \norm{AX - B}_F
\]
\subsubsection{i)}
show that the SVD of $A$ takes the form
\[
A = U\begin{bmatrix}\si \\ 0\end{bmatrix}V^T
\]
\partbreak
\begin{solution}

    We are given that $A$ is full column rank, thus full rank. Thus all singular values of $A > 0$. We then have two choices for $\si '$. Either $\si '$ is ``fat", where the zero matrix appears to the right of the singular values, or $\si'$ is ``skinny," meaning the zero matrix appear below. In either case, $\si'$ is $m \times n$, meaning it retains the same shape as $A$. Since $A$ is full \textit{column} rank, that means $\si'$ spans the entirety of $n$, so we are left with $\si'$ being the skinny case, which is the one shown.
\end{solution}
\subsubsection{ii)}
Show that 
\[
\norm{AX - B}_F^2 = \norm{\si Y - C_1}_F^2 + \norm{C_2}_F^2
\]
where $Y = V^TXV$ and $C = \begin{bmatrix}C_1\\C_2\end{bmatrix} = U^TBV$.
\partbreak
\begin{solution}

    We first note that the Frobenius norm is unitary invariant. That is, multiplying the argument inside by a unitary matrix does nothing to the result. If we multiply by $V$, the right eigenvectors of $A$, we get the following:
\newpage
    \alignbreak
    \begin{align}
        \norm{AX - B}_F^2 &= \norm{AXV - BV}_F^2 &\text{(Justified above.)}\nonumber\\
        &= \norm{U\si ' V^TXV - BV}_F^2 &\text{(SVD of $A$.)}\nonumber\\
        &= \tr \Big[ (U\si' Y - BV)^T(U\si' Y - BV) \Big] &\text{(Frobenius trace and $Y$ definition.)}\nonumber\\
        &= \tr \Big[ Y^T\si'^T U^TU\si 'Y - Y^T\si'^TU^TBV] &\text{(Distribution.)}\nonumber\\
        &= \tr \Big[ Y^T\si'^T\si^T Y - Y^T\si'^TC - C^T\si' Y + V^TB^TUU^TBV\Big] &\text{(Definition of $C$.)}\nonumber\\
        &= \tr \Big[ Y^T\si^T \si^T Y - Y^T\si ^TC_1 - C_1^T\si^TY + C^TC\Big] &\text{(Form of $\si'$ and $C$.)}\nonumber\\
        &= \tr \Big[ (\si Y)^T\si Y - (\si Y)^TC_1 - C_1^T \si Y + C_1^TC_1 + C_2^TC_2\Big] &\text{(Rewriting and form of $C$.)}\nonumber\\
        &= \tr \Big[ (\si Y - C_1)^T(\si Y - C_1)\Big] + \tr \Big[ C_2^TC_2\Big] &\text{(Trace is linear.)}\nonumber\\
        &=\norm{\si Y - C_1}_F^2 + \norm{C_2}_F^2 &\text{(Frobenius trace.)}\nonumber
    \end{align}
    \alignbreak
\end{solution}
\newpage
    \subsubsection{iii)}
    Note that $Y$ must be symmetric if $X$ is. Show that
    \[
    \norm{\si Y - C_1}_F^2 = \sum_{i = 1}^n |\si_iy_{ii} - c_{ii}|^2 + \sum_{j > i} (|\sigma_iy_{ij} - c_{ij}|^2 + |\sigma_jy_{ij} - c_{ji}|^2 
    \]

    and deduce that the minimum value above is attained when
    \[
    y_{ij} = \frac{\sigma_ic_{ij} + \sigma_jc_{ji}}{\sigma_i^2 + \sigma_j^2}
    \]
    for all $i, j = 1, ..., n$.
    \partbreak
    \begin{solution}

        This can be shown by the Frobenius norm and symmetry of $Y$.
{\small
        \alignbreak
        \begin{align}
            \norm{\si Y - C_1}_F^2 &= \sum_{i = 1}^n\sum_{j = 1}^n |\sigma_iy_{ij} - c_{ij}|^2 &\text{(Definition of Frobenius norm.)}\nonumber\\
            &= \sum_{i = 1}^n\Bigg[ \sum_{j = 1}^{i - 1} |\sigma_iy_{ij} - c_{ij}|^2 + |\sigma_iy_{ii} - c_{ii}|^2  + \sum_{j = i+1}^n |\sigma_iy_{ij} - c_{ij}|^2\Bigg] &\text{(Breaking $j$ sum up.)}\nonumber\\
            &= \sum_{i = 1}^n|\sigma_iy_{ii} - c_{ii}|^2 + \sum_{i = 1}^n\Bigg[ \sum_{j = 1}^{i - 1} |\sigma_iy_{ij} - c_{ij}|^2 + \sum_{j = i+1}^n |\sigma_iy_{ij} - c_{ij}|^2\Bigg]&\text{(Distribution.)}\nonumber\\
            &= \sum_{i = 1}^n|\sigma_iy_{ii} - c_{ii}|^2 + \sum_{i = 1}^n\Bigg[ \sum_{j = i+1}^{n} |\sigma_jy_{ji} - c_{ji}|^2 + \sum_{j = i+1}^n |\sigma_iy_{ij} - c_{ij}|^2\Bigg]&\text{(Transposition.)}\nonumber\\
            &= \sum_{i = 1}^n|\sigma_iy_{ii} - c_{ii}|^2 + \sum_{j > i}\Big[ |\sigma_jy_{ij} - c_{ji}|^2 + |\sigma_iy_{ij} - c_{ij}|^2\Big]&\text{(Symmetry of $Y$ and simplifying.)}\nonumber
        \end{align}
}%

\alignbreak
\newpage
We should expect the minimal symmetric $X$ to have only diagonal terms. Thus, we set the terms of the second sum to be equal zero. Thus, the following can be shown:

\alignbreak
\begin{align}
    0 &= (\sigma_jy_{ij} - c_{ji})^2 + (\sigma_iy_{ij} - c_{ij})^2 \nonumber\\
    &= \sigma_j^2 y_{ji}^2 + c_{ji}^2 - 2\sigma_jy_{ij}c_{ji} + \sigma_i^2y_{ij}^2 + c_{ij}^2 - 2\sigma_iy_{ij}c_{ij} &\text{(Expanding.)}\nonumber\\
    &= y_{ij}^2(\sigma^2_j + \sigma^2_i) - 2(\sigma_jc_{ji} + \sigma_ic_{ij})y_{ij} + (c_{ij}^2 + c_{ji}^2) &\text{(Grouping, polynomial in $y_{ij}$.)}\nonumber
\end{align}
\alignbreak

We can note that this polynomial is strictly positive, as this was how it was constructed. Then the minimum of the polynomial is at the vertex, $-b/2a$, meaning,

\[
y_{ij} = \frac{\sigma_jc_{ji} + \sigma_ic_{ij}}{\sigma_i^2 + \sigma_j^2}
\]

which is what we wanted to show. 
\end{solution}

\newpage
\section{Problem 2, part c}
Now emulate the previous part to find the rank-$r$ matrix $X \in \R^{m \times n}$ that solves 

\[
\min_{\text{rank}(X) \leq r}\norm{AX - B}_F.
\]
\newpage
\section{Problem 3}
Let $A \in \C^{m\times n}$ and $\textbf{b} \in \C^m$. We will discuss a variant of $A\textbf{x} \approx \textbf{b}$ where the error occurs only in $A$. Note that in ordinary least squares we assume that the error occurs only in \textbf{b} while in total least squares we assume that it occurs in both $A$ and \textbf{b}.

\subsection{Problem 3, part a}
Show that if $0 \neq \textbf{x} \in \C^n$, then
\[
\norm{A\Big( \id - \frac{\textbf{x}\textbf{x}^*}{\textbf{x}^*\textbf{x}}\Big)}_F^2 = \norm{A}_F^2 - \frac{\norm{A\textbf{x}}_2^2}{\textbf{x}^*\textbf{x}}
\]
\partbreak
\begin{solution}

 Taking the left side, the following steps can be shown:

\vspace{-5mm}
 \alignbreak
 {\small
 \begin{align}
    \norm{A\Big( \id - \frac{\textbf{x}\textbf{x}^*}{\textbf{x}^*\textbf{x}}\Big)}^2_F &= \tr \Big[ \Big( A - \frac{1}{\textbf{x}^*\textbf{x}}A\textbf{x}\textbf{x}^*\Big)^*\Big( A - \frac{1}{\textbf{x}^*\textbf{x}}A\textbf{x}\textbf{x}^*\Big)\Big] &\text{(Frobenius trace.)}\nonumber\\
    &= \tr \Big[ A^*A\Big] - \frac{1}{\textbf{x}^*\textbf{x}} \tr \Big[ A^*A\textbf{x}\textbf{x}^*\Big] - \frac{1}{\textbf{x}^*\textbf{x}} \tr \Big[ (A\textbf{x}\textbf{x}^*)^*A\Big] + \frac{1}{(\textbf{x}^*\textbf{x})^2}\tr\Big[ (A\textbf{x}\textbf{x}^*)^*A\textbf{x}\textbf{x}^* \Big] &\text{(Expanding.)}\nonumber\\
    &= \norm{A}_F^2 - \frac{1}{\textbf{x}^*\textbf{x}} \tr \Big[ A^*A\textbf{x}\textbf{x}^*\Big] - \frac{1}{\textbf{x}^*\textbf{x}} \tr \Big[ \textbf{x}\textbf{x}^*A^*A\Big] + \frac{1}{(\textbf{x}^*\textbf{x})^2}\tr\Big[ \textbf{x}\textbf{x}^*A^*A\textbf{x}\textbf{x}^* \Big] &\text{(Hermiting(?).)}\nonumber\\
    &= \norm{A}_F^2 - \frac{1}{\textbf{x}^*\textbf{x}} \tr \Big[ \textbf{x}^*A^*A\textbf{x}\Big] - \frac{1}{\textbf{x}^*\textbf{x}} \tr \Big[ \textbf{x}^*A^*A\textbf{x}\Big] + \frac{1}{(\textbf{x}^*\textbf{x})^2}\tr\Big[ A^*A\textbf{x}\textbf{x}^* \textbf{x}\textbf{x}^*\Big] &\text{(Trace permutating.)}\nonumber\\
    &= \norm{A}_F^2 - \frac{1}{\textbf{x}^*\textbf{x}} \tr \Big[ (A\textbf{x})^*A\textbf{x}\Big] - \frac{1}{\textbf{x}^*\textbf{x}} \tr \Big[ (A\textbf{x})^*A\textbf{x}\Big] + \frac{1}{(\textbf{x}^*\textbf{x})^2}\tr\Big[ A^*A\textbf{x}(\textbf{x}^* \textbf{x})\textbf{x}^*\Big] &\text{(Hermiting, associating.)}\nonumber\\
    &= \norm{A}_F^2 - \frac{1}{\textbf{x}^*\textbf{x}} \norm{A\textbf{x}}_2^2 - \frac{1}{\textbf{x}^*\textbf{x}} \norm{A\textbf{x}}_2^2 + \frac{1}{\textbf{x}^*\textbf{x}}\tr\Big[ A^*A\textbf{x}\textbf{x}^*\Big] &\text{(Trace of scalar is scalar.)}\nonumber\\
     &= \norm{A}_F^2 - \frac{1}{\textbf{x}^*\textbf{x}} \norm{A\textbf{x}}_2^2 - \frac{1}{\textbf{x}^*\textbf{x}} \norm{A\textbf{x}}_2^2 + \frac{1}{\textbf{x}^*\textbf{x}}\tr\Big[ \textbf{x}^*A^*A\textbf{x}\Big] &\text{(Permutating.)}\nonumber\\
     &= \norm{A}_F^2 - \frac{1}{\textbf{x}^*\textbf{x}} \norm{A\textbf{x}}_2^2 - \frac{1}{\textbf{x}^*\textbf{x}} \norm{A\textbf{x}}_2^2 + \frac{1}{\textbf{x}^*\textbf{x}}\norm{A\textbf{x}}_2^2 &\text{(Trace of scalar is scalar.)}\nonumber\\
     &=\norm{A}_F^2 - \frac{\norm{A\textbf{x}}_2^2}{\textbf{x}^*\textbf{x}} &\text{(Simplifying.)}\nonumber
 \end{align}
 }%
 \alignbreak

\end{solution}

\newpage
\subsection{Problem 3, part b}
Show that the matrix 
\[
E = \frac{(\textbf{b} - A\textbf{x})\textbf{x}^*}{\textbf{x}^*\textbf{x}} \in \C^{m\times n}
\]
has the smallest 2-norm among all $E \in \C^{m\times n}$ that satisfy 
\[
(A + E) \textbf{x} = \textbf{b}
\]
\partbreak
\begin{solution}

 Note that all $E$'s satisfy $E\textbf{x} = \textbf{b} - A\textbf{x}$. To distinguish the $E$ matrices, we'll denote $E'$ as a matrix that satisfies $(A+E')\textbf{x} = \textbf{b}$. Thus, the following can be shown:

 \alignbreak
 \begin{align}
     \norm{E}_2^2 &= \norm{\frac{(\textbf{b} - A\textbf{x})\textbf{x}^*}{\textbf{x}^*\textbf{x}}}^2_2 &\text{(Given.)}\nonumber\\
     &= \Big| \frac{1}{\textbf{x}^*\textbf{x}}\Big|^2 \norm{(\textbf{b} - A\textbf{x})\textbf{x}^*}_2^2 &\text{(Denominator is real.)}\nonumber\\
     &\leq \Big| \frac{1}{\textbf{x}^*\textbf{x}}\Big|^2 \norm{\textbf{b} - A\textbf{x}}_2^2\norm{\textbf{x}^*}_2^2 &\text{(Cauchy Schwartz.)}\nonumber\\
     &= \frac{\norm{\textbf{b} - A\textbf{x}}_2^2}{\norm{\textbf{x}}^2_2} &\text{(Simplifying.)}\nonumber\\
     &= \frac{\norm{E'\textbf{x}}_2^2}{\norm{\textbf{x}}_2^2} &\text{(Equality above.)}\nonumber\\
     &\leq \norm{E'}_2^2 \frac{\norm{\textbf{x}}_2^2}{\norm{\textbf{x}}_2^2} &\text{(Consistency of 2-norm.)}\nonumber\\
     \implies \norm{E}_2^2&\leq \norm{E'}_2^2 &\text{(Simplifying.)}\nonumber\\
     \iff \norm{E}_2 &\leq \norm{E'}_2 \nonumber
 \end{align}
 \alignbreak
\end{solution}

\newpage
\subsection{Problem 3, part c}
Let $A, \textbf{b}, \textbf{x}$ be given and fixed. What are the solutions of 
\[
    \min_{(A+E)\textbf{x} = \textbf{b}} \norm{E}_2 \hspace{3mm} \text{and} \hspace{3mm} \min_{(A+E)\textbf{x} = \textbf{b}} \norm{E}_F
\]
where the minimum is taken over all $E \in \C^{m\times n}$ such that $(A+E)\textbf{x} = \textbf{b}$?
\partbreak
\begin{solution}

We just showed in the previous part that when 
\[
    E = \frac{(\textbf{b} - A\textbf{x})\textbf{x}^*}{\textbf{x}^*\textbf{x}} \in \C^{m\times n}
\] 

 Then $E$ is the smallest of all $E$'s satisfying $(A+E)\textbf{x} = \textbf{b}$. We can then just plug this value for $E$ in to get the minimum value for each.

 \alignbreak
 \begin{align}
     \min_{(A+E)\textbf{x} = \textbf{b}} \norm{E}_2 &= \norm{\frac{(\textbf{b} - A\textbf{x})\textbf{x}^*}{\textbf{x}^*\textbf{x}}}_2 &\text{(Plugging in.)}\nonumber\\
     &= \frac{1}{\norm{\textbf{x}}_2^2}\norm{\textbf{b}\textbf{x}^* - A\textbf{x}\textbf{x}^*}_2 &\text{(Simplifying.)}\nonumber\\
     \min_{(A+E)\textbf{x} = \textbf{b}} \norm{E}_F &= \frac{1}{\norm{\textbf{x}}_2^2}\tr\Big[ (\textbf{b}\textbf{x}^* - A\textbf{x}\textbf{x}^*)^*(\textbf{b}\textbf{x}^* - A\textbf{x}\textbf{x}^*)\Big] &\text{(Frobenius trace.)}\nonumber\\
     &= \frac{1}{\norm{\textbf{x}}_2^2}\tr\Big[ \textbf{x}\textbf{b}^*\textbf{b}\textbf{x}^* + \textbf{x}\textbf{x}^*A^*A\textbf{x}\textbf{x}^* - \textbf{x}\textbf{b}^*A\textbf{x}\textbf{x}^* - \textbf{x}\textbf{x}^*A^*\textbf{b}\textbf{x}^*\Big] &\text{(Hermiting.)}\nonumber\\
     &= \frac{1}{\norm{\textbf{x}}_2^2}\tr\Big[ \norm{\textbf{b}}_2^2\textbf{x}\textbf{x}^* + A^*A\textbf{x}\textbf{x}^*\textbf{x}\textbf{x}^* - \textbf{b}^*A\textbf{x}\textbf{x}^*\textbf{x} - \textbf{x}^*A^*\textbf{b}\textbf{x}^*\textbf{x}\Big] &\text{(Permuting.)}\nonumber\\
     &= \frac{1}{\norm{\textbf{x}}_2^2}\tr\Big[ \norm{\textbf{b}}_2^2\norm{\textbf{x}}_2^2 + \norm{\textbf{x}}_2^2\norm{A\textbf{x}}_2^2 - \norm{\textbf{x}}_2^2\textbf{b}^*A\textbf{x} - \norm{\textbf{x}}_2^2\textbf{x}^*A^*\textbf{b}\Big] &\text{(Taking norms.)}\nonumber\\
     &= \tr\Big[ \norm{\textbf{b}}_2^2 + \norm{A\textbf{x}}_2^2 - \textbf{b}^*A\textbf{x} - \textbf{x}^*A^*\textbf{b}\Big] &\text{(Simplifying.)}\nonumber\\
     &= \norm{\textbf{b}}_2^2 + \norm{A\textbf{x}}_2^2 - 2\tr \Big[ \textbf{b}^*A\textbf{x}\Big] &\text{(Simplifying.)}\nonumber
 \end{align}
 \alignbreak
\end{solution}

\newpage
\subsection{Problem 3, part d}
Given $\textbf{a} \in \C^n, \textbf{b} \in \C^m$, and $\delta > 0$. Show how to solve the problems 
\[
\min_{\norm{E}_F \leq \delta} \norm{E\textbf{a} - \textbf{b}}_2 \hspace{5mm} \text{and} \hspace{5mm} \max_{\norm{E}_F \leq \delta} \norm{E\textbf{a} - \textbf{b}}_2
\]
\partbreak
\begin{solution}
    
\end{solution}
\newpage
\section{Problem 4}
In the following, $\kappa(A) := \norm{A}\norm{A^\dag}$ for $A\in \C^{m\times n}$ where $\norm{\cdot}$ denotes a submultiplicative matrix norm. We will write $\kappa_p(A)$ if the norm involved is a matrix p-norm. 

\subsection{Problem 4, part a}
Show that for any nonzero $A \in \C^{m\times n}$, $\kappa(A) \geq 1$.

\partbreak
\begin{solution}

 We are given that $\norm{\cdot}$ is submultiplicative, thus $\norm{A}\norm{A^\dag} \geq \norm{AA^\dag}.$ If A is invertible, then $AA^\dag = AA^{-1} = \id$. Note that $\norm{\id} = \norm{\id^2} \leq \norm{\id}^2$, thus $\norm{\id} \geq 1$ for any submultiplicative norm. Then $\kappa(A) = \norm{A}\norm{A^\dag} \geq 1$. If $A$ is not invertible, then we can use SVD. We will write $A = U\Sigma V^*$, so $A^\dag = V\Sigma^\dag U^*$. Then,

 \alignbreak
 \begin{align}
     \norm{AA^\dag} &= \norm{U\Si V^* V \Si ^\dag U^*} &\text{(By SVD.)}\nonumber\\
     &= \norm{U\Si\Si ^\dag U^*} &\text{($V$ is hermitian.)}\nonumber\\
     &= \norm{U\begin{bmatrix}\id_r\\0\end{bmatrix}U^*}, \norm{U\begin{bmatrix}\id_r &0\end{bmatrix}U*} &\text{(Depends if $m \geq n$, or reverse.)}\nonumber\\
     &= \norm{U_rU^*} &\text{(In either case, we take the first r vectors of $U$.)}\nonumber\\
     &= \norm{\id_r} &\text{(Simplifying.)}\nonumber\\
     &\geq 1 &\text{(Identity is $\geq 1$.)}\nonumber
 \end{align}
 \alignbreak

 Note in these steps, I took the first $r$ rows \textit{or} columns from $U$ or $U^*$. Without loss of generality, since $U$ is hermitian, the above is given. Note that $\id_r$ in the last step is an $m \times n$ matrix, which I take its norm as in the $r\times r$ case. 
\end{solution}

\newpage
\subsection{Problem 4, part b}
Show that for any $A \in \C^{m\times n}$,
\[
    \kappa_2(A^*A) = \kappa_2(A)^2
\]
but that in general,
\[
    \kappa(A^*A)\neq \kappa(A)^2
\]
\partbreak
\begin{solution}

We know that $A^*A$ will have eigenvalues $|\lm|^2$ for eigenvalue $\lm$ of A. Thus, $\norm{A^*A}_2 = \max \{ |\lm|\} = (\max \{ |\lm | \})^2$. Similarly, $(A^*A)^{-1}$ has eigenvalues $1/|\lm|^2$. Note that $A^*A$ is positive \textit{semi}-definite, so any eigenvalue of $A^*A$ is $\geq 0$. If there is an eigenvalue equal zero, then $\kappa_2(A^*A) = \infty$. This also happens for $\kappa(A)$, since $\max \{ 1/|\lm|\} = \infty$. If we still take the SVD of $A^*A$, just to be safe, then 
\[
\kappa_2(A^*A) = \norm{A^*A}_2\norm{(A^*A)^{-1}}_2 = \frac{(\max{|\lm|})^2}{(\max{1/|\sigma^2|})} = (\max{|\lm|})^2 (\min{|\sigma|})^2
\]
Here we cannot assume that $A$ is invertible, but by SVD, we can say what the 2-norm of $A^{-1}$ is. This will then be the largest singular value of $\si ^\dag$, which is the smallest singular value of $\si$. Thus when squaring, we get
\[
\kappa_2(A)^2 = (\norm{A}_2\norm{A^\dag}_2)^2 = (\max\{|\lm|\})^2(\min \{ \sigma \} )^2
\]

Thus, these two are equivalent. This should hold true for any norm, by the equivalence of norms over a finite dimensional vector space.  
\end{solution}

\newpage
\subsection{Problem 4, part c}
Show that for nonsingular $A, B \in \C^{n\times n}$, 
\[
\kappa(AB)\leq \kappa(A)\kappa(B).
\]
 It this true in general without the nonsingular condition?
 \partbreak
 \begin{solution}

     For nonsingular $A, B$, this property is easy to show. Note the pseudoinverse turns into a normal inverse for the product.

     \alignbreak
     \begin{align}
         \kappa(AB) &= \norm{AB}\norm{(AB)^{-1}} &\text{(Given.)}\nonumber\\
         &= \norm{AB} \norm{B^{-1}A^{-1}} &\text{(Inverse of $AB$.)}\nonumber\\
         &\leq \norm{A} \norm{B} \norm{B^{-1}} \norm{A^{-1}} &\text{(Submultiplication.)}\nonumber\\
         \implies \kappa(AB) &\leq \kappa(A)\kappa(B)\nonumber
     \end{align}
     \alignbreak
 \end{solution}

\newpage
 \subsection{Problem 4, part d}
 Let $Q \in \C^{n\times n}$ be a matrix with orthonormal columns. Show that 
 \[
 \kappa_2(Q) = 1.
 \]
 Is this true if $Q$ has orthonormal rows instead? Is this true with $\kappa_1$ or $\kappa_\infty$ in place of $\kappa_2$?
 \partbreak
 \begin{solution}
     First, note that the 2-norm is unitary invariant, Since $Q^* = Q^{-1}$, then 
     \[
     \kappa_2(Q) = \norm{Q}_2 \norm{Q^{-1}}_2 = \norm{Q}_2\norm{Q^T}_2 = \norm{\id}_2\norm{\id}_2 = 1
     \]

     I wrote the above equality to emphasize that this will work is $Q$ has orthonormal rows instead, since the $Q$ and $Q^T$ terms will just swap around. Since $\kappa_1$ is taken with the one matrix norm, it will take the max absolute row sum of $Q$, which is 1 since $Q$ has orthonormal columns. This is the same for $\kappa_\infty$, as again the $Q$ and $Q^T$ terms will just swap.  
 \end{solution}

 \subsection{Problem 4, part e}
 Let $R \in \C^{n\times n}$ be a nonsingular upper-triangular matrix. Show that
 \[
 \kappa_\infty(R) \geq \frac{\max_{i = 1, ..., n}|r_{ii}|}{\min_{i = 1, ..., n}|r_{ii}|}
 \]
 \partbreak
 Note for an upper triangular matrix, its eigenvalues are on the diagonal, so we can note the numerator of the right hand side is just $\norm{R}_2$. If we can accept that the diagonal of the inverse of $R$ is just the inverted diagonal of $R$, then 
 \[
 \min |r_{ii}| = \max 1/|r_{ii}| = \max 1/|\lm_i| = \norm{R^{-1}}_2
 \]

 That is, since the upper triangular matrices form a closed subgroup of $GL(n)$, then we can say the diagonal or $R^{-1}$ is the inverse of the diagonal of $R$. We already know that $R^{-1}$ will have eigenvaules $1/\lm_i$ for $\lm_i$ of $R$. They are all nonzero, so this is valid. So the right hand side is just $\kappa_2(R)$. We just need to show that 
 \[
 \kappa_\infty(R) \geq \kappa_2(R).
 \]
 By the equivalence of norms, $\norm{R}_2 \leq \sqrt{n}\norm{R}_\infty$ for any positive $n$, so $\norm{R}_2 \leq \norm{R}_\infty$. This argument also holds for $R^{-1}$. Thus, $\kappa_2(R) \leq \kappa_\infty(R)$. 

\newpage
 \subsection{Problem 4, part f}
 Show that for any nonsingular $A \in \C^{n\times n}$, 
 \[
 \kappa(A) \leq \max \Bigg\{ \frac{\norm{AX - \id}}{\norm{XA - \id}}, \frac{\norm{XA - \id}}{\norm{AX - \id}} \Bigg\}
 \]
 \partbreak
 \begin{solution}
     
 \end{solution}
\end{document}
