\input{definitions}

% Enter the specific assignment number and topic of that assignment below, and replace "Your Name" with your actual name.
\title{STAT 31020: Homework 7}
\author{Caleb Derrickson}
\date{February 28, 2024}

\begin{document}
\onehalfspacing
\maketitle
\allowdisplaybreaks

\tableofcontents

\newpage
\section{Problem 1}
\subsection{Problem 1, part 1}
Prove that in this circumstance (random matrix $A$, $e = (1, 1, ..., 1)\T \in \R^n$, $b = Ae$, random objective $c = e + 100*u $) you can readily determine a strictly feasibly point in the set defined in 14.12(b). Determine a value for that ensures that this point belongs to the neighborhood $\scN_{-\infty}(\gamma)$. You will use this point as a starting point and the corresponding value gamma in algorithm 14.2. 
\partbreak
\begin{solution}

    The strictly feasible set is defined as 
    \[\scF^0 = \{(x, \lm, s) : Ax = b, \ A\T \lm + s = c, \ (x, s) > 0\}\]
    Since we have that $b$ is formed by the vector of all ones ($e$) applied to A, we can easily see that $x = e$ is known. Moreover, we can partition $c$ so that $s = 100*u$, and $A\T \lm = e$. Since we don't know that $A$ is invertible (let alone square), we cannot readily solve the system. However, since $A$ is full row rank, then the matrix $AA\T$ is not only square, but invertible. Multiplying by $A$ on both sides gives $AA\T \lm= Ae = b$, which implies $\lm = (AA\T)\inv b$, which can be readily computed. \par

    \jump
    Next, we will determine a value for $\gamma$ which ensures our point $(x, \lm, s)$ is within $\scN_{-\infty}(\gamma)$. Note that
    \[\scN_{-\infty}(\gamma) = \{(x, \lm, s) \in \scF ^0 : x_is_i \geq \gamma \mu \text{ for all } i = 1, 2, ..., n\}\]
    By the calculations done above, we have that 
    \[x = e, \quad s = 100*u\]
    Therefore, in order for our point to be in $\scN_{-\infty}(\gamma)$ for some gamma, we need that $x_is_i \geq \gamma\mu$ for all $i$. The form of $\mu$ is given as 
    \[\mu = \frac{1}{n}x\T s = \frac{100}{n}e\T u = \frac{100}{n}\sum_i u_i = 100 * u_{avg}.\]
    Moreover, $x_is_i = 100 * u_i$ for all $i$, we therefore need $\gamma$ to satisfy $\gamma \leq \frac{u_i}{u_{avg}}$ for all $i$, so pick the smallest of such values. Then 
    \[\gamma = \min\left\{ \frac{u_i}{u_{avg}} : i = 1, 2, ..., n\right\}.\]
    We need to show that $\gamma \in (0, 1]$. Since $u$ is a random vector whose elements are computed over a uniform distribution over (0, 1), we have that $u_i \in (0, 1)$ for all $i$. Hence the smallest element is in $(0, 1)$. Note $\gamma$ cannot be negative by this construction. Furthermore, since $u_i \in (0, 1)$, then by uniform distribution, $u_{avg} \approx \frac{1}{2}$, with equality held when $n \to \infty$. Since $u_{avg} \in(0, 1)$, then $\min (u_{i})/2 \in (0, 1)$. Therefore, $\gamma \in (0, 1]$.   
\end{solution}

\newpage
\subsection{Problem 1, part 2}
Implement the long step interior point method from the textbook. (Algorithm 14.2) starting from the feasible strictly interior point above. Apply it at n=2m, at 10 random instances and report the mean and standard deviation of the time it took to solve the problem; Do it for m=5,10, â€¦. for as high as your computation is tolerable on your computer (overall run time for this exercise should not be more than 1 hour once the parameters are chosen). Compare the time to solution with the one of simplex (use the code from last time) for the same size (e.g. by plotting both means on the same graph for the ranges you have data for both). Experiment with the parameters (i.e. the size of the centering parameter and the parameter defining the neighborhood size) within the ranges suggested by the book.

\newpage
\section{Problem 2}
Find the range of parameters $\mu$ for which the problem 18.12 has a solution whose component $x$ is the same as the solution of the problem 18.10, assuming that the latter satisfies LICQ and strong second-order conditions. In particular, this will elucidate how penalty parameters should be chosen. 
\partbreak
\begin{solution}

    For reference, I will include the entirety of 18.10 and 18.12 below.
    \quotebreak
    \vspace{-9mm}
    \begin{multicols}{2}
    \begin{quote}
        \underline{\textbf{18.10}}:
        \begin{align*}
            & \min_x f(x)                \\[-1.5ex]
            &\text{subject to}:                     \\[-1.5ex]
            &\hspace{20mm} c_i(x) = 0, \ i \in \scE \\[-1.5ex]
            &\hspace{20mm} c_i(x) \geq 0, \ i \in \scI
        \end{align*}
    \end{quote}
    \columnbreak
    \begin{quote}
        \underline{\textbf{18.12}}:
        \begin{align*}
            & \min_{x, v, w, t} f(x) + \mu \sum_{i \in \scE}(v_i + w_i) + \mu \sum_{i \in \scI} t_i        \\[-1.5ex]
            &\text{subject to}: \\[-1.5ex]
            &\hspace{20mm} c_i(x) = v_i - w_i, \ i \in \scE, \\[-1.5ex]
            &\hspace{20mm} c_i(x) \geq -t_i, \hspace{7mm} i \in \scI, \\[-1.5ex]
            &\hspace{20mm} v, w, t \geq 0
        \end{align*}
    \end{quote}
    \end{multicols}
    \vspace{-12mm}\quotebreak

    Here, we impose $\mu > 0$, since if $\mu$ were less than zero, we would then wish to max out our penalty terms, which is not equivalent to the original problem. If $\mu = 0$, then the penalty terms $v, w, t$ would contribute nothing to the original problem, but would be chosen as free parameters, which is not exactly the original problem. Hence, we enforce $\mu > 0$. We need to get a tighter bound than this, which can be found via the KKT conditions on 18.12. We will assume that KKT holds, since the necessary conditions for KKT are met, as well as it being a requirement for second order optimality. We first write the Lagrangian of 18.12. First, for the sake of brevity, denote $z = (x, v, w, t)$, and $\delta = (\lambda, \alpha, \beta, \gamma)$. The new parameters, $\alpha, \beta, \gamma$, will come from positivity of the minimizing variables $v, w, t$. Furthermore, to distinguish the Lagrangian from problem 18.10 from the Lagrangian in 18.12, we will denote them respectively as $\scL_1, \scL_2$. The Lagrangian is now
    \[\scL_2(z, \delta) = f(x) + \mu\sum_{i \in \scE} (v_i + w_i) + \mu \sum_{i \in \scI} t_i - \sum_{i \in \scE} \lm_i (c_i(x) - v_i + w_i) - \sum_{i \in \scL} \lm_i (c_i(x) + t_i) - \alpha\T v - \beta\T w - \gamma\T t\]
    If we want to condense these down into a clearer equation, we will introduce the indicator vectors, $\ind_\scE, \ind_\scI$, which will have value 1 if the index element is in the denoted set, and zero otherwise. The Lagrangian now becomes
    \[\scL_2(z, \delta) = f(x) + \mu\ind_\scE\T (v + w) + \mu\ind_\scI\T t - \ind_\scE\T(\lm (c(x) - v + w)) - \ind_\scI\T (\lm (c(x) + t)) - \alpha\T v - \beta\T w - \gamma\T t\]

    \newpage
    KKT conditions of 18.12 then tell us the following (assuming some optimal solution $(z\star, \delta\star)$):

    \tightalignbreak
    \vspace{-6mm}\begin{align}
        &\grad_z \scL_2(z\star, \delta\star) = 0,                                     \label{KKT1}\\[-1.5ex]
        &c_i(x\star) -(v_i\star - w_i\star) = 0,                                      \label{KKT2}\\[-1.5ex]
        &0 \leq c_i(x\star) \geq - t_i\star,                                          \label{KKT3}\\[-1.5ex]
        &(v\star, w\star, t\star) \geq 0,                                             \label{KKT4}\\[-1.5ex]
        &\delta\star \geq 0,                                                          \label{KKT5}\\[-1.5ex]
        &\lm_i\star(c_i(x\star) + t_i\star) = 0,         \hspace{10mm}\forall \ i \in \scI \label{KKT6}\\[-1.5ex]
        &\alpha_i\star v_i\star = 0,                \hspace{27mm}\forall \ i \in \scE \label{KKT7}\\[-1.5ex]
        &\beta_i\star w_i\star = 0,                 \hspace{26mm}\forall \ i \in \scE \label{KKT8}\\[-1.5ex]
        &\gamma_i\star t_i\star = 0,                \hspace{28mm}\forall \ i \in \scI \label{KKT9}
    \end{align}
    \vspace{-15mm}\alignbreak

    We should first take into consideration with which entries of $\lm$ we are affecting. We see in the Lagrangian itself, when we take the gradient with respect to $z$, we will receive different parts of the constraint vector. To this end, I will denote $\lm = \lm^1 \oplus \lm^2,$ where $\lm^1 \in \R^{|\scE|}$, and $\lm^2 \in \R^{|\scI|}$. Writing our the first condition, we have the following:
    \begin{align*}
        \grad_z\scL(z\star, \delta\star) = \mqty[\grad_x\scL_1(x\star, \lm\star)\\\mu\ind + \lm^1 - \alpha\\ \mu\ind - \lm^1 - \beta\\\mu\ind - \lm^2 - \gamma] = \mqty[0\\0\\0\\0] \implies \mqty[\grad_x\scL_1(x\star, \lm\star) = 0\\\mu = \alpha_i - \lm_i^1, \ i \in \scE\\ \mu = \beta_i + \lm_i^1, \ i \in \scE \\ \mu = \gamma_i + \lm^2_i, \ i \in \scI]
    \end{align*}
    The first condition aligns with the KKT conditions of 18.10. Adding the second and third lines implies that 
    \[\mu = \frac{\alpha_i + \beta_i}{2}, \ i \in \scE.\]
    Therefore, $\mu$ is fixed upon choice of $\alpha$ and $\beta$. Taking the fourth row, we can multiply each side by $t_i\star$ to get $\mu t_i\star = \gamma_i\star t_i\star + \lm_i^2 t_i\star$. From (\ref{KKT2}), we see that $w_i\star = v_i\star$ for all $i$. In accordance with the first equation's KKT results, we have the $\lm_i\star c_i(x\star) = 0$ for $i \in \scI$. Therefore, (\ref{KKT6}) simply states that $\lm_i\star t_i\star = 0$. Furthermore, from (\ref{KKT9}), we can see that $\mu t_i\star = 0$, which implies that $t_i\star = 0$ since $\mu > 0$. Next, observe the following:
    \[\mu(v_i\star + w_i\star) = \mu v_i\star + \mu w_i\star = \alpha_i\star v_i\star - \lm_i\star v_i\star + \beta_i\star w_i\star + \lm_i\star w_i\star = \lm_i\star(w_i\star - v_i\star)\]
    Here, we distributed the $\mu$ and took different values of $\mu$ from the gradient term $(\ref{KKT1})$. But by, (\ref{KKT2}), we have that $\mu(v_i\star + w_i\star) = 0$, therefore $v_i\star = -w_i\star$. BUT we also have that they are equal as well. Since this can only happen when both are zero, we have that $v_i\star, w_i\star = 0$. Note that this is true because we assumed KKT conditions hold for 18.10, which allows us to prove these penalty terms are zero. 

    \newpage
    Finally, observe the system of values for $\mu$ from the gradient term. 
    \begin{align*}
        &\alpha_i  = \mu + \lm^1_i, \ i \in \scE\\
        &\beta_i = \mu - \lm_i^1, \ i \in \scE\\
        &\gamma_i = \mu - \lm_i^2, \ i \in \scI
    \end{align*}
    By the logic provided above, we found that these parameters are free (but fixed), since $v_i\star = w_i\star = t_i\star = 0$. However, by (\ref{KKT5}), they are positive. Thus, to prevent any term to be negative, we require that our choice of $\mu$ should be greater than any $\lm_i$, $i \in \scI \cup \scE$. Therefore, I claim that 
    
    \[\mu \geq \max\{\lm_i : i \in \scE \cup \scI\}\]
    
    is a valid bound. In order for this to hold, we need to show that the second order conditions are held for 18.12 as well. In particular, we have that 
    \[u\T\grad^2_{zz}\scL_2(z\star, \delta\star) u > 0 \text{ for all } u \in \mathcal{C}(z\star, \delta\star),\]
    where $\mathcal{C}$ denotes the critical cone at $(z\star, \delta\star)$. For reference, the general definition for an element to be in the critical cone is the following:
    \[u \in \mathcal{C}(x\star, \lm\star) \iff \begin{cases}
    \grad_x c_i(x\star)\T w = 0 &\text{ for } i \in \scE\\
    \grad_x c_i(x\star)\T w = 0 &\text{ for } i \in \scI \cap \scA \text{ with } \lm_i\star > 0\\
    \grad_x c_i(x\star)\T w \geq 0 &\text{ for } i \in \scI \cap \scA \text{ with } \lm_i\star = 0\\ 
    \end{cases}
    \]
    The constraint equations will come from the KKT conditions. This will then give
    \[\mqty[u_1\\u_2\\u_3\\u_4] \in \mathcal{C}(z\star, \delta\star) \iff \begin{cases}
        \grad_x c_i(x\star)\T u_i^1 - u^2_i + u^3_i = 0, &\text{ for } i \in \scE\\
        \grad_xc_i(x\star)\T u^1 + u^4_i = 0 \ (\geq 0), &\text{ for } \lm_i\star > 0 \ ( = 0) \text{ for all } i \in \scI \cap \scA\\
        u_i^2 = 0 \ ( \geq 0) &\text{ for } \alpha_i\star > 0 \ (= 0) \text{ for all } i \in \scE\\
        u_i^3 = 0 \ (\geq 0) &\text{ for } \beta_i\star > 0 \ (= 0) \text{ for all } i \in \scE\\
        u_i^4 = 0 \ (\geq 0) &\text{ for } \gamma_i\star > 0 \ (= 0) \text{ for all } i \in \scI
    \end{cases}\]

    Quick note: since $\alpha_i = \mu + \lm_i^1$, we have that $\alpha_i > 0$, (since $\mu > 0$). Then $u_i^2 = 0$. We only issue we see is when either condition for $\gamma_i, \beta_i = 0$. This is avoided IF we restrict $\mu$ even further, so that 
    \[\mu > \max\{ \lm_i : i \in \scE \cup \scI\}.\]
    When this is the case, we have that $\beta_i\star > 0, \gamma_i\star > 0$, which implies that $u_i^3 = u_i^4 = 0$. Therefore, the critical cone for 18.12 aligns with the critical cone for 18.10.

    \newpage
    Finally, we need to see that the second order sufficient conditions hold for 18.12. We have already calculated the gradient of the Lagrangian for 18.12 when analyzing the KKT conditions. We can take the hessian from there. Note that the last three rows are constants, so their derivative is zero. Thus, we see the following:
\[
\grad_{zz}^2 \scL(z\star, \delta\star) = \mqty[
    \grad_{xx}^2\scL(x\star, \lm\star) & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0
]
\]

    Thus, 
    \[u\T\grad^2_{zz}\scL_2(z\star, \delta\star) u = u_1\T\grad^2_{xx}\scL_2(x\star, \lm\star) u_1 > 0,\]
    by assumption that 18.10 satisfies the second order sufficient conditions. 
\end{solution}

\newpage
\section{Problem 3}
Consider the following linear program, which contains "free variables" denoted by $y$
\[\min c\T x + d\T y, \text{ subject to } A_1x + A_2y = b, \ x \geq 0.\]
By introducing Lagrange multipliers $\lm$ for the equality constraints $s$ for the bounds $x \geq 0$, write down the optimality conditions for this problem in analogous fashion to (14.3). Following (14.4) and (14.16), use the conditions to derive the general step equations for a primal-dual interior-point method. Express these equations in an augmented system form analogously to (14.42) and explain why it is not possible to reduce further to a formulation like (14.44) in which the coefficient matrix is symmetric positive definite. 
\partbreak
\begin{solution}

    To begin, we should analyze the conditions which KKT provides for the given problem (This is 14.3). First, we need to form the Lagrangian, which is explicitly
    \[\scL(x, y, \lm, s) = c\T x + d\T y - \lm\T (A_1x + A_2 y - b) - s\T x,\]
    where $\lm$ is the slack term for the equality constraints, and $s$ is the slack term for the inequality constraints. Next, define $z = (x, y)$: the KKT conditions thus yield:
    \begin{align*}
        \grad_z \scL(x, y, \lm, s) =& \mqty[\grad_x \scL\\\grad_y \scL] = \mqty[c - A_1\T\lm - s\\d - A_2\T \lm] = \mqty[0\\0],\\
        A_1x + A_2y =& b, \\
        (x, s) \geq& 0,\\
        \lm (A_1x + A_2y - b) =& 0,\\
        s_ix_i =& 0, \quad i = 1, ..., n 
    \end{align*}
    Next, we need to construct the mapping $F$, given as 14.4. Following the book, we have that
    \begin{align*}
        F(x, y, \lm, s)=& \mqty[c - A_1\T \lm - s\\ d - A_2\T \lm\\A_1x + A_2y - b\\XSe ] = 0\\
        &(x, s, \lm) \geq 0
    \end{align*}
    Here, the matrices $X$ and $S$ are equivalent to that in the book. That is, $X = $ diag($x$), $S = $ diag($s$). Next, we aim to take the Jacobian of $F$. This will be done in the same manner as in class, where we advance each term by $\Delta$, cut off all nonlinear terms, and set equal to zero. The procedure is detailed below. For the last row, I will instead demonstrate the procedure for some element $x_is_i$, $i \leq n$.

    \alignbreak
    \begin{align*}
        &F(x + \Delta x, y + \Delta y, \lm + \Delta\lm, s + \Delta s) = \mqty[c - A_1\T \lm - A_1\T\Delta\lm - s - \Delta s\\d - A_2\T \lm - A_2\T \Delta\lm\\A_1x + A_1\Delta x + A_2 y + A_2 \Delta y - b\\x_is_i + x_i\Delta s_i + \Delta x_i s_i + \Delta x_i \Delta s_i]\\
        &= \mqty[c - A_1\T \lm - A_1\T\Delta\lm - s - \Delta s\\d - A_2\T \lm - A_2\T \Delta\lm\\A_1x + A_1\Delta x + A_2 y + A_2 \Delta y - b\\x_is_i + x_i\Delta s_i + \Delta x_i s_i] = \mqty[0\\0\\0\\0]\\
        &\hspace{12mm}\implies \mqty[0&0&A_1\T&\id\\0&0&A_2\T&0\\A_1&A_2&0&0\\S&0&0&X]\mqty[\Delta x\\\Delta y\\\Delta \lm \\\Delta s] = \mqty[c - s - A_1\T\lm\\d - A_2\T \lm\\b - A_2y - A_1x\\SXe - \sigma\mu e] = \mqty[-r^1_c\\-r_c^2\\-r_b\\-r_{xs}]
    \end{align*}
    \alignbreak
    Where the final term $\sigma \mu e$ represents the central path term. I tried keeping the same notation as the book, but since we are minimizing over two variables, I had to superscript the $r_c$ terms to differentiate them. Let us then solve for the last row for $\Delta s$ in order to eliminate it. The last equation implies 
    \[S\Delta x + X\Delta s = -r_{xs},\]
    which can be simplified by the following:
    \newcommand{\h}{^{1/2}}
    \newcommand{\mh}{^{-1/2}}
    \alignbreak
    \begin{align*}
        &S\Delta x + X\Delta s = -r_{xs}\\
        &S\h S\h \Delta x + X\h X\h \Delta s = -r_{xs}\\
        &X\h S\h (S\h X\mh \Delta x + S\mh X\h \Delta s) = -r_{xs}\\
        &S\h X\mh \Delta x + S\mh X\h \Delta s = -X\mh S\mh r_{xs}\\
        &D\inv \Delta x + D \Delta s = -X\mh S\mh r_{xs}\\
        &D \Delta s = -( X\mh S\mh r_{xs} + D\inv \Delta x) \\
        &\Delta s = -(D\inv X\mh S\mh r_{xs} + D^{-2}\Delta x)\\
        &\Delta s = -(S\h X\mh X\mh S\mh  r_{xs} + D^{-2}\Delta x)\\
        &\Delta s = -X\inv r_{xs} - X\inv S \Delta x
    \end{align*}
    \alignbreak

    Where $D$ is defined in the same way as in the book. Then, we can add $-X\inv$ times the last row to the first equation to eliminate the last row. This operation is done only on the first since we want the last column of the matrix to be only zeros. We then get:
    \begin{align*}
        \mqty[-X\inv S &0 &A_1\T \\ 0&0&A_2\T\\A_1&A_2&0]&\mqty[\Delta x\\\Delta y\\ \Delta \lm] = \mqty[-r_c^1 + X\inv r_{xs}\\ -r_c^2\\-r_b]\\
        &\Delta s = -X\inv r_{xs} - X\inv S\Delta x 
    \end{align*}
    This is analogous to the augmented system in the book (14.42). Note that, it is not possible to reduce any more, since this matrix has the possibility to contain zero columns. Note that, we can arbitrarily assume that the matrices $A_1$ and $A_2$ are full row rank (by preconditioning). Hence, the second (block) column could potentially have linearly dependent (individual) columns. Hence, the kernel of our matrix is nonzero. This was not a problem for the case done in the book, since $A$ is assumed full row, and $A\T$ is then full column, implying the coefficient matrix has both full row and full column ranks. Therefore, we cannot go further. 

\end{solution}
\end{document}