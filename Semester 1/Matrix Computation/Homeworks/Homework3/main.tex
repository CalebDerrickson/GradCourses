\input{definitions}

% Enter the specific assignment number and topic of that assignment below, and replace "Your Name" with your actual name.
\title{STAT 30900: Homework 3}
\author{Caleb Derrickson}
\date{November 11, 2023}

\begin{document}
\onehalfspacing
\maketitle
\allowdisplaybreaks
{\color{cit}\vspace{2mm}\noindent\textbf{Collaborators:}} The TA's of the class, as well as Kevin Hefner, and Alexander Cram.

\tableofcontents

\newpage
\section{Problem 1}
Given a symmetric $A \in \R^{n\times n}, \textbf{0} \neq \textbf{x} \in \R^n$, and $\textbf{b} \in \R^n$. Let 
\[
\textbf{r} = \textbf{b} - A\textbf{x}
\]
Consider the $QR$ decomposition
\[
[\textbf{x}, \textbf{r}] = QR
\]
and observe that if $E\textbf{x} = \textbf{r}$, then
\[
(Q^TEQ)(Q^T\textbf{x}) = Q^T\textbf{r}.
\]
Show how to compute a symmetric $E \in \R^{n\times n}$ so that is attains 
\[
\min_{(A+E)\textbf{x} = \textbf{b}}\norm{E}_F
\]
where the minimum is taken over all symmetric $E$.

\newpage
\section{Problem 2}
Let $A \in \R^{m \times n}$ and suppose the complete orthogonal decomposition is given by
\[
A = Q_1 \begin{bmatrix}L &0\\0&0\end{bmatrix}Q_2^T
\]
where $Q_1$ and $Q_2$ are orthogonal, and $L$ is a nonsingular lower triangular matrix. Recall that $X \in \R^{n \times m}$ is the unique pseudo-inverse of $A$ is the following Moore-Penrose conditions hold:
\begin{enumerate}[(i)]
    \item $AXA = A$ 
    \item $XAX = X$ 
    \item $(AX)^T = AX$ 
    \item $(XA)^T = XA$
\end{enumerate}
and in which case we write $A^\dag = X$.
\subsection{Problem 2, part a}
Let 
\[
A^- = Q_2\begin{bmatrix}L^\inv &Y \\ 0 &0\end{bmatrix}Q_1^T, \hspace{5mm} Y\neq 0. 
\]
Which of the four conditions (i) - (iv) are satisfied?
\partbreak
\begin{solution}

    We will go straight into calculations.
    \alignbreak
    \begin{align}
        i) \ AXA &=  Q_1 \begin{bmatrix}L &0\\0&0\end{bmatrix}Q_2^T Q_2\begin{bmatrix}L^\inv &Y \\ 0 &0\end{bmatrix}Q_1^T Q_1 \begin{bmatrix}L &0\\0&0\end{bmatrix}Q_2^T &\text{(Given.)}\nonumber\\
        &= Q_1 \begin{bmatrix}L &0\\0&0\end{bmatrix}\begin{bmatrix}L^\inv &Y \\ 0 &0\end{bmatrix}\begin{bmatrix}L &0\\0&0\end{bmatrix}Q_2^T &\text{($Q_1, Q_2$ are orthogonal.)}\nonumber\\
        &=  Q_1 \begin{bmatrix}\id &LY\\0&0\end{bmatrix}\begin{bmatrix}L &0 \\ 0 &0\end{bmatrix}Q_2^T &\text{(Matrix Multiplication.)}\nonumber\\
        &=  Q_1 \begin{bmatrix}L &0 \\ 0 &0\end{bmatrix}Q_2^T &\text{(Matrix Multiplication.)}\nonumber\\
        &= A &\text{(By Definition.)}\nonumber\\
        ii) \ XAX &= Q_2\begin{bmatrix}L^\inv &Y \\ 0 &0\end{bmatrix}Q_1^T Q_1 \begin{bmatrix}L &0\\0&0\end{bmatrix}Q_2^T Q_2\begin{bmatrix}L^\inv &Y \\ 0 &0\end{bmatrix}Q_1^T &\text{(Given.)}\nonumber\\
        &= Q_2\begin{bmatrix}L^\inv &Y \\ 0 &0\end{bmatrix} \begin{bmatrix}L &0\\0&0\end{bmatrix}\begin{bmatrix}L^\inv &Y \\ 0 &0\end{bmatrix}Q_1^T &\text{($Q_1, Q_2$ are orthogonal.)}\nonumber\\
        &= Q_2\begin{bmatrix}\id &0\\0 &0\end{bmatrix}\begin{bmatrix}L^\inv &Y \\0 &0\end{bmatrix}Q_1^T &\text{(Matrix Multiplication.)}\nonumber\\
        &= Q_2\begin{bmatrix}L^\inv &Y \\0 &0\end{bmatrix}Q_1^T&\text{(Matrix Multiplication.)}\nonumber\\
        &= X &\text{(By Definition.)}\nonumber\\
        iii) \ (AX)^T &= \Bigg(Q_1 \begin{bmatrix}L &0\\0&0\end{bmatrix}Q_2^T Q_2\begin{bmatrix}L^\inv &Y \\ 0 &0\end{bmatrix}Q_1^T \Bigg)^T &\text{(Given.)}\nonumber\\
        &= \Bigg(Q_1 \begin{bmatrix}L &0\\0&0\end{bmatrix}\begin{bmatrix}L^\inv &Y \\ 0 &0\end{bmatrix}Q_1^T \Bigg)^T &\text{($Q_2$ is orthogonal.)}\nonumber\\
        &= \Bigg( Q_1\begin{bmatrix}\id &LY \\ 0 &0\end{bmatrix}Q_1^T\Bigg)^T &\text{(Matrix multiplication.)}\nonumber\\
        &=  Q_1\begin{bmatrix}\id &0 \\ LY &0\end{bmatrix}Q_1^T &\text{(Transposition.)}\nonumber\\
        &\neq AX &\text{(As can be seen.)}\nonumber\\
        iv) \ (XA)^T &= \Bigg( Q_2 \begin{bmatrix}L^\inv &Y \\ 0&0\end{bmatrix}Q_1^TQ_1\begin{bmatrix}L &0\\0 &0\end{bmatrix}Q_2^T\Bigg)^T &\text{(Given.)}\nonumber\\
        &= \Bigg( Q_2 \begin{bmatrix}L^\inv &Y \\ 0&0\end{bmatrix}\begin{bmatrix}L &0\\0 &0\end{bmatrix}Q_2^T\Bigg)^T &\text{($Q_1$ is orthogonal.)}\nonumber\\
        &= \Bigg( Q_2 \begin{bmatrix}\id &0 \\ 0 &0\end{bmatrix}Q_2^T\Bigg)^T &\text{(Matrix Multiplication.)}\nonumber\\
        &= Q_2 \begin{bmatrix}\id &0 \\ 0 &0\end{bmatrix}Q_2^T &\text{(Transposition.)}\nonumber\\
        &= XA &\text{(As can be seen.)}\nonumber
    \end{align}
    \alignbreak
    Thus, we see that $(i), (ii),$ and $(iv)$ hold, but not $(iii)$.
\end{solution}

\newpage
\subsection{Problem 2, part b}
Prove that 
\[
A^\dag = Q_2 \begin{bmatrix}L^\inv &0 \\ 0 &0\end{bmatrix}Q_1^T
\]
by letting 
\[
A^\dag = Q_2\begin{bmatrix}X_{11} &X_{12} \\ X_{21} &X_{22}\end{bmatrix}Q_1^T
\]
and by completing the following steps
\begin{itemize}
    \item Using $(i)$, prove that $X_{11} = L^\inv$.
    \item Using the symmetry conditions $(iii)$ and $(iv)$, prove that $X_{12} = X_{21} = 0$. 
    \item Using $(ii)$, prove that $X_{22} = 0$.
\end{itemize}
\partbreak
\begin{solution}

    Here we are letting the middle term take on any form (within reason), then arguing by the properties of the Moore-Penrose pseudo-inverse, that it must take this form. Then, let 
    \[
    A^\dag = Q_2\begin{bmatrix}X_{11} &X_{12} \\ X_{21} &X_{22}\end{bmatrix}Q_1^T
    \]

    We will then recover terms by the above properties in the suggested order.
    \begin{itemize}
        \item By the first property, $AXA = A$ must be obeyed. Then, 
        \vspace{-5mm}
        \alignbreak
        \vspace{-5mm}
        \begin{align}
            AA^\dag A &= Q_1\begin{bmatrix}L &0\\0&0\end{bmatrix}Q_2^TQ_2\begin{bmatrix}X_{11} &x_{12}\\X_{21} &X_{22}\end{bmatrix}Q_1^TQ_1\begin{bmatrix}L &0\\0&0\end{bmatrix} &\text{(Given.)}\nonumber\\
            &=  Q_1\begin{bmatrix}L &0\\0&0\end{bmatrix}\begin{bmatrix}X_{11} &x_{12}\\X_{21} &X_{22}\end{bmatrix}\begin{bmatrix}L &0\\0&0\end{bmatrix} &\text{($Q_1, Q_2$ are orthogonal.)}\nonumber\\
            &= Q_1 \begin{bmatrix}LX_{11} &LX_{12}\\ 0&0\end{bmatrix}\begin{bmatrix}L &0\\0&0\end{bmatrix}Q_2^T &\text{(Matrix multiplication.)}\nonumber\\
            &= Q_1\begin{bmatrix}LX_{11}L &0 \\0&0\end{bmatrix}Q_2^T &\text{(Matrix multiplication.)}\nonumber\\
            \implies &LX_{11}L = L &(AA^\dag A = A.)\nonumber\\
            \iff &X_{11} = L^\inv &\text{($L$ is nonsingular.)}\nonumber
        \end{align}
        \alignbreak
        \vspace{-5mm}
        \newpage
        \item Next, we will take the symmetric properties of the Moore-Penrose pseudo inverse.
        \alignbreak
        \begin{align}
        AA^\dag &= \Bigg( Q_1\begin{bmatrix}L &0\\0 &0\end{bmatrix}Q_2^TQ_2\begin{bmatrix}L^\inv &X_{12}\\ X_{21} &X_{22}\end{bmatrix}Q_1^T\Bigg)^T &\text{(Given.)}\nonumber\\
        &= \Bigg( Q_1\begin{bmatrix}L &0\\0 &0\end{bmatrix}\begin{bmatrix}L^\inv &X_{12}\\ X_{21} &X_{22}\end{bmatrix}Q_1^T\Bigg)^T &\text{($Q_2$ is orthogonal.)}\nonumber\\
        &= \Bigg( Q_1\begin{bmatrix}\id &LX_{12}\\0 &0\end{bmatrix}Q_1^T\Bigg)^T &\text{(Matrix multiplication.)}\nonumber\\
        \implies LX_{12} &= L &\text{(Transposition and $(AA^\dag)^T = AA^\dag$.)}\nonumber\\
        \implies \ \  X_{12} &= 0 &\text{($L$ is nonsingular, thus nonzero.)}\nonumber\\
        (A^\dag A)^T &= \Bigg( Q_2 \begin{bmatrix}L^\inv &0 \\X_{21} &X_{22}\end{bmatrix}Q_1^TQ_1\begin{bmatrix}L &0\\0&0\end{bmatrix}Q_2^T\Bigg)^T &\text{(Given.)}\nonumber\\
        &= \Bigg( Q_2 \begin{bmatrix}L^\inv &0 \\X_{21} &X_{22}\end{bmatrix}\begin{bmatrix}L &0\\0&0\end{bmatrix}Q_2^T\Bigg)^T &\text{($Q_1$ is orthogonal.)}\nonumber\\
        &= \Bigg( Q_2 \begin{bmatrix}\id &0 \\X_{21}L &0\end{bmatrix}Q_2^T\Bigg)^T &\text{(Matrix multiplication.)}\nonumber\\
        \implies X_{21}L &= 0 &\text{(Transposition and $(A^\dag A)^T = A^\dag A$.)}\nonumber\\
        \implies \ \ X_{21} &= 0 &\text{(L is invertible thus nonzero.)}\nonumber
        \end{align}
        \alignbreak
        
        \item Finally, we will show $X_{22} = 0$.
        \alignbreak
        \begin{align}
            A^\dag AA^\dag &= Q_2\begin{bmatrix}L^\inv &0 \\ 0 &X_{22}\end{bmatrix}Q_1^TQ_1 \begin{bmatrix}L &0\\ 0 &0\end{bmatrix}Q_2^TQ_2 \begin{bmatrix}L^\inv &0 \\0 &X_{22}\end{bmatrix}Q_1^T &\text{(Given.)}\nonumber\\
            &= Q_2\begin{bmatrix}L^\inv &0 \\ 0 &X_{22}\end{bmatrix} \begin{bmatrix}L &0\\ 0&0\end{bmatrix} \begin{bmatrix}L^\inv &0 \\0 &X_{22}\end{bmatrix}Q_1^T &\text{($Q_1, Q_2$ are orthogonal.)}\nonumber\\
            &= Q_2\begin{bmatrix}\id &0\\0&0\end{bmatrix}\begin{bmatrix}L^\inv &0 \\ 0&X_{22}\end{bmatrix}Q_1^T &\text{(Matrix multiplication.)}\nonumber\\
            &= Q_2\begin{bmatrix}L^\inv &0 \\0&0\end{bmatrix}Q_1^T &\text{(Matrix multiplication.)}\nonumber\\
            \implies X_{22} &= 0 &(A^\dag AA^\dag = A^\dag.) \nonumber
        \end{align}
        \alignbreak
    \end{itemize}

    Therefore, $A^\dag$ is of the given form. Note that I passed over some steps, including steps where I equated entries only in the block matrix, ignoring the factors which would appear when multiplied by $Q_i$ for its transpose. Since they are orthonormal however, these factors will cancel out, leaving us only with the entry in the block matrix. 
\end{solution}

\newpage
\section{Problem 3}
Let $A \in \R^{m \times n}, \textbf{b} \in \R^m,$ and $\textbf{c}\in \R^n$. We are interested in the least squares problem
\begin{align}
    \min_{\textbf{x}\in \R^n} \norm{A\textbf{x} - \textbf{b}}^2_2 \label{p3: min}
\end{align}
\subsection{Problem 3, part a}
Show that \textbf{x} is a solution to (\ref{p3: min}) if and only if \textbf{x} is a solution to the \textit{augmented system}
\begin{align}
    \begin{bmatrix}\id &A\\A^T&0\end{bmatrix}\begin{bmatrix}\textbf{r}\\\textbf{x}\end{bmatrix} = \begin{bmatrix}\textbf{b}\\0\end{bmatrix} \label{p3: augmented soln}
\end{align}
\partbreak
\begin{solution}

    \begin{itemize}
        \item $\underline{\implies}):$ Suppose (\ref{p3: min}) $\implies$ (\ref{p3: augmented soln}) were false, that is, $\textbf{x}$ is not a solution to the augmented system. Note that since $\textbf{x}$ is a solution to (\ref{p3: min}), then $A\textbf{x} - \textbf{b} = \textbf{d}$, for some $\textbf{d}$. Note that $\textbf{d}$ is in general nonzero. For simplicity, we will break this proof down into cases. 

        \begin{itemize}
            \item \underline{Case 1:} $\textbf{b} \in$ im $(A)$.

            Then the minimum of $\norm{A\textbf{x} - \textbf{b}}$ would equal zero, since $\norm{A\textbf{x - \textbf{b}}} = \norm{A(\textbf{x} - \textbf{y})} = 0$, which attains minimum when $\textbf{x} = \textbf{y}$, meaning $A\textbf{x} = \textbf{b}$. Then $\textbf{d} = 0$. Then (\ref{p3: augmented soln}) simplifies to
            \[
            \begin{bmatrix}\id &A\\ A^T& 0\end{bmatrix}\begin{bmatrix}\textbf{0}\\ \textbf{x}\end{bmatrix} = \begin{bmatrix}\textbf{b}\\0\end{bmatrix}.
            \]
            This implies $A\textbf{x} = b$ and $A^T\textbf{0} = \textbf{0}$. Since these are both true, then we run into a contradiction.
            \item \underline{Case 2:} $\textbf{b} \notin$ im $(A)$

            Then $\norm{A\textbf{x} - b} \neq 0$. This would mean $\textbf{d} \neq 0$, and $\textbf{d} \neq $ im$(A)$ since if it were, then we can write $\textbf{d} = A\textbf{y}$, then $\norm{A(\textbf{x} - \textbf{y}) - b} = 0$, which means $\textbf{x} - \textbf{y}$ is a solution to (\ref{p3: min}), meaning $\textbf{x}$ is not a minimum, which is a contradiction. Since $\textbf{d} \notin$ im $(A)$, then $\textbf{d} \in \ker (A^T)$ by the Fredholm Alternative. Then (\ref{p3: augmented soln}) is equivalent to
            \[
            \begin{bmatrix}\id &A\\ A^T& 0\end{bmatrix}\begin{bmatrix}\textbf{d}\\ \textbf{x}\end{bmatrix} = \begin{bmatrix}\textbf{b}\\0\end{bmatrix}.
            \]
            This implies $A\textbf{x} + \textbf{d} = \textbf{b}$ and $A^T\textbf{d} = 0$. This is is true, since we only know $\textbf{d}$ up to sign. Thus (\ref{p3: augmented soln}) holds, giving us a contradiction. 
        \end{itemize}

        \item $\underline{\impliedby}):$

        Note that (\ref{p3: augmented soln}) is equivalent to $A\textbf{x} + \textbf{b} = \textbf{r}$ and $A^T\textbf{r} = 0$. We will again break this down into cases. 

        \begin{itemize}
            \item \underline{Case 1:} $\textbf{b} \in $ im$(A)$. 

            Then $\textbf{b} = A\textbf{y}$ for some $\textbf{y} \in \R^n$. Thus $\textbf{r}= A\textbf{x} + \textbf{b} = A(\textbf{x} - \textbf{y})$. Thus when taking the minimum over all $\textbf{x}$, we see that $\norm{A(\textbf{x} - \textbf{y})} = 0$, exactly when $\textbf{x} = \textbf{y}$. Since $\norm{\cdot}_2$ is positive, then $\textbf{x} \in \argmin (\norm{A\textbf{x} - \textbf{b}})$, which means $\textbf{x}$ is a solution to (\ref{p3: min}).

            \item \underline{Case 2:} $\textbf{b}\notin$ im$(A)$.

            Then, by the Fredholm Alternative, $\textbf{b} \in \ker(A^T)$. Thus, the following can be shown:
            \alignbreak
            \begin{align}
                \norm{\textbf{r}}_2^2 &= \norm{A\textbf{x} - \textbf{b}}_2^2 = (A\textbf{x} - \textbf{b})^T(A\textbf{x} - \textbf{b}) &\text{(2-norm, given.)}\nonumber\\
                &= \textbf{x}^TA^TA\textbf{x} - \textbf{x}^TA^T\textbf{b} - \textbf{b}^TA\textbf{x} + \textbf{b}^T\textbf{b} &\text{(Factoring out.)}\nonumber\\
                &= \textbf{x}^TA^TA\textbf{x}  - \textbf{b}^TA\textbf{x} + \textbf{b}^T\textbf{b} &(\textbf{b} \in \ker(A^T).)\nonumber\\
                &= \textbf{x}^TA^TA\textbf{x}  - (A^T\textbf{b})^T\textbf{x} + \textbf{b}^T\textbf{b} &\text{(Associativity.)}\nonumber\\
                &= \textbf{x}^TA^TA\textbf{x} + \textbf{b}^T\textbf{b} &(\textbf{b} \in \ker(A^T).)\nonumber\\
                &= \norm{A\textbf{x}}_2^2 + \norm{\textbf{b}}_2^2 &\text{(2-norm definition.)}\nonumber
            \end{align}
            \alignbreak

            This then means that $\textbf{x}$ satisfies the Pythagorean Theorem, implying that $\textbf{x}$ is a solution to (\ref{p3: min}). 
        \end{itemize}
    \end{itemize}
\end{solution}

\newpage
\subsection{Problem 3, part b}
Show that the $(m + n) \times (m + n)$ matrix in (\ref{p3: augmented soln}) is nonsingular if and only if $A$ has full column rank.
\partbreak
\begin{solution}

    Denote $\A$ to be the $(m + n) \times (m + n)$ matrix in (\ref{p3: augmented soln}). We will again break this down into cases. 

    \begin{itemize}
        \item $\underline{\implies}):$ 

        Suppose false, that is, $\A$ is singular, but $A$ has full column rank. Note this means $A$ has full rank. Since $\A$ is singular, then $\exists \textbf{v} \in \R^{m + n} \setminus \{\textbf{0}\}$ which is mapped to $\textbf{0}$ under $\A$. Let $\textbf{v} = [\textbf{v}_\textbf{r}, \textbf{v}_\textbf{x}]^T$. This then means, by (\ref{p3: augmented soln}), that  $\textbf{v}_\textbf{r} + A\textbf{v}_\textbf{x} = 0$ and $A^T\textbf{v}_\textbf{r} = 0$. We can substitute the first equation into the second to get $A^TA\textbf{v}_\textbf{x} = 0$. Note that if $A$ is full rank, then $A^T$ is also full rank. This means that $A\textbf{v}_\textbf{x} \in \ker(A^T)$. Since $A^T$ has full rank, this means $\textbf{v}_\textbf{x} \in \ker(A)$, which means $\textbf{v}_\textbf{x} =0 $, since $A$ is full rank. Then, since $A\textbf{v}_\textbf{x} = -\textbf{v}_\textbf{r}, \implies \textbf{v}_\textbf{r} = 0$ as a result. Note we assumed that $\textbf{v} \neq 0$, which is a contradiction, since $\textbf{v}$ was found to only be zero. 

        \item $\underline{\impliedby}):$

        This is equivalent to showing $\det (\A) \neq 0$. Note that in general, a $2\times 2$ block matrix has determinant
        \[
        \det \Bigg( \begin{bmatrix}A &B\\C &D\end{bmatrix}\Bigg) = \det(A)\det(D - CA^\inv B)
        \]

        With $\A$, this then means $\det(\A) = \det(\id)\det(0 - A^T(\id)^\inv A) = det(-A^TA) = (-1)^n\det(A^TA)$. Note that $A^TA$ has full rank when $A$ has full rank. Furthermore, $A^TA$ is normal, thus its SVD decomposition coincides with an eigenvalue decomposition, with $\sigma^2 = \lm$. since $A$ has full rank, then all singular values of $A^TA$ are nonzero, thus all eigenvalues are nonzero, thus $\det(A^TA) \neq 0$. Therefore $\det(\A) \neq 0$ when $A$ has full column rank. 
    \end{itemize}
\end{solution}

\newpage
\subsection{Problem 3, part c}
Suppose $A$ has full column rank and the $QR$ decomposition of $A$ is 
\[
A = Q\begin{bmatrix}R\\0\end{bmatrix}.
\]
Show that he solution to the augmented system
\[
\begin{bmatrix}\id &A\\A^T &0\end{bmatrix}\begin{bmatrix}\textbf{y}\\\textbf{x}\end{bmatrix}
=
\begin{bmatrix}\textbf{b}\\\textbf{c}\end{bmatrix}
\]
can be computed from
\[
\textbf{x} = (R^\inv)^T \textbf{c}, \hspace{5mm} \begin{bmatrix}\textbf{d}_1\\\textbf{d}_2\end{bmatrix} = Q^T\textbf{b},
\]

and
\[
\textbf{x} = R^\inv(\textbf{d}_1 - \textbf{z}), \hspace{5mm }\textbf{y} = Q\begin{bmatrix}\textbf{z}\\\textbf{d}_2\end{bmatrix}.
\]
\partbreak
\begin{solution}

    This is just an exercise in reverse engineering, and computation. We can recover the first equation in the following steps:

    \alignbreak
    \begin{align*}
        Q^T\textbf{b} &= \mqty[\textbf{d}_1\\\textbf{d}_2] &\text{(Given.)}\\
        &= \mqty[\textbf{d}_1 + \textbf{z} - \textbf{z}\\\textbf{d}_1] &\text{(Adding a zero.)}\\
        &= \mqty[\textbf{z}\\\textbf{d}_2] + \mqty[\textbf{d}_1 - \textbf{z}] &\text{(Separating.)}\\
        &= \mqty[\textbf{z}\\\textbf{d}_2] + \mqty[R\\0] R^\inv(\textbf{d}_1 - \textbf{z}) &(R^\inv R = \id.)\\
        &= Q^T\Bigg(Q\mqty[\textbf{z}\\\textbf{d}_2]\Bigg) + \mqty[R\\0] R^\inv(\textbf{d}_1 - \textbf{z}) &(Q^TQ = \id.)\\
        \implies Q^T\textbf{b} &= Q^T\textbf{y} + \mqty[R\\0]\textbf{x} &\text{(Given definitions.)}\\
        \implies \textbf{b} &= \textbf{y} + Q\mqty[R\\0] \textbf{x} &\text{(Multiplying by $Q$.)}\\
        \implies \textbf{b} &= \textbf{y} + A\textbf{x} &\text{($QR$ of $A$.)}
    \end{align*}
    \alignbreak

    The second equation can also be found similarly. 
    \alignbreak
    \begin{align*}
        \textbf{z} &= (R^\inv)^T\textbf{c} &\text{(Given.)}\\
        \implies \ \ \textbf{z}^T &= \textbf{c}^TR^\inv &\text{(Transposition.)}\\
        \implies \ \ \textbf{c}^T &= \textbf{z}^TR &\text{(Multiplying by $R$.)}\\
        \implies  \textbf{c} &= R^T\textbf{z} &\text{(Transposition.)}\\
        &= \mqty[R^T&0] \mqty[\textbf{z}\\\textbf{d}_2] &\text{(Block Equivalence, element of $\textbf{d}_2$ could be anything.)}\\
        &= \mqty[R^T&0] Q^TQ\mqty[\textbf{z}\\\textbf{d}_2] &(Q^TQ = \id.)\\
        &= \Bigg( Q\mqty[R\\0]\Bigg)^TQ\mqty[\textbf{z}\\\textbf{d}_2] &\text{(Transposition.)}\\
        \implies \textbf{y} &= A^T\textbf{y} &\text{(Given definitions.)}
    \end{align*}
    \alignbreak
\end{solution}

\newpage
\subsection{Problem 3, part d}
Hence deduce that if $A$ has full column rank, then 
\[
A^\dag = R^\inv Q^T_1
\]
where $Q = [Q_1, Q_2]$ with $Q_1 \in \R^{m\times n}$ and $Q_2 \in \R^{m\times (m - n)}$. Check this agrees with the general formula derived for a rank-retaining factorization $A = GH$ in the lectures. 

\partbreak
\begin{solution}

    We first show that the given $A^\dag$ satisfies the properties of the Moore-Penrose pseudo-inverse:

    \alignbreak
    \begin{align*}
        i) \ AA^\dag A &= (Q\mqty[R\\0]R^\inv Q_1^T)A &\text{(Given.)}\\
        &= \mqty[Q_1\\Q_2] \mqty[R\\0]R^\inv Q_1^T)A &\text{(Definitions.)}\\
        &=  Q_1RR^\inv Q_1^T A &\text{(Matrix multiplication.)}\\
        &= (Q_1Q_1^T)A &\text{($R^\inv R = \id$.)}\\
        &= (Q_1Q_1^T)\mqty[Q_1&Q_2]\mqty[R\\0] &\text{(Writing out.)}\\
        &= Q_1Q_1^TQ_1R &\text{(Matrix Multiplication.)}\\
        &= Q_1R &\text{($Q_1$ has orthonormal columns.)}\\
        ii) \ A^\dag A A^\dag &= R^\inv Q_1^T\mqty[Q_1\\Q_2]\mqty[R\\0]A^\dag &\text{(Given.)}\\
        &= R^\inv Q_1^TQ_1R A^\dag &\text{(Matrix multiplication.)}\\
        &= (\id)A^\dag &\text{($Q_1$ has orthonormal columns, $R^\inv R = \id$.)}\\
        iii) \ (A^\dag A)^T &= \Bigg( R^\inv Q_1^T \mqty[Q_1 &Q_2]\mqty[R\\0]\Bigg)^T &\text{(By definition.)}\\
        &= (R^\inv Q_1^TQ_1R)^T &\text{(Matrix multiplication.)}\\
        &= (R^\inv R)^T &\text{($Q_1$ has orthonormal columns.)}\\
        &= \id &\text{($R^\inv R = \id$)}\\
        iv) \ (AA^\dag)^T &=\Bigg(\mqty[Q_1 &Q_2]\mqty[R\\0]R^\inv Q_1^T\Bigg)^T &\text{(Given.)}\\
        &= \Bigg(Q_1RR^\inv Q_1^T\Bigg)^T &\text{(Matrix multiplication.)}\\
        &= \Bigg(Q_1 Q_1^T\Bigg)^T  &(R^\inv R = \id.)\\
    \end{align*}
    \alignbreak

    Note that I have to be careful in handling the $Q$ subspaces. Since $Q$ had orthonormal columns, $Q_1^T Q_1 = \id$, but $Q_1Q_1^T \neq \id$ in general. However, $(Q_1Q_1^T)^T = Q_1Q_1^T$, Thus $(iv)$ will hold. This issue also pops ups in $(i)$, but problems subside once we multiply on the right by $A$. Thus $(i) - (iv)$ hold. We need to show that this then agrees with the rank-retaining lectures as shown in class. That is, 
    \[
    A^\dag = H^T(HH^T)^\inv (G^TG)^\inv G^T
    \]
    We need to be careful in our choice of $G$ and $H$, since if $H = \mqty[R\\0]$, then 
    \[
    HH^T = \mqty[R\\0]\mqty[R^T&0] =\mqty[RR^T &0\\0&0],
    \]
    which is not invertible. Thus taking this with respect to the \textit{condensed} $QR$ decomposition, then take $H = R, G = Q_1$. Then the following steps are justified:

    \alignbreak
    \begin{align*}
        A^\dag &= H^T(HH^T)^\inv (G^TG)^\inv G^T &\text{(Given.)}\\
        &= R^T(RR^T)^\inv (Q_1^TQ_1)^\inv Q_1^T &\text{(Plugging in.)}\\
        &= R^T(R^T)^\inv R^\inv (\id)^\inv Q_1^T &\text{($(AB)^\inv = B^\inv A^\inv$ if defined.)}\\
        &= R^\inv Q_1^T &(R^T(R^T)^\inv = \id.)
    \end{align*}
    \alignbreak

    Thus, we find our $A^\dag$ to coincide with the one found using the general formula for a rank-retaining factorization.
\end{solution}

\newpage
\section{Problem 4}
Let $A \in \R^{m \times n}$. Suppose we apply $QR$ factorization with column pivoting to obtain the decomposition
\[
A = Q\mqty[R&S\\0&0]\Pi^T
\]
where $Q$ is orthogonal and $R$ is upper triangular and invertible. Let $\textbf{x}_B$ be the \textit{basic solution}, i.e.,
\[
\textbf{x}_B = \Pi \mqty[R^\inv &0\\0&0]Q^T\textbf{b},
\]
and let $\hat{\textbf{x}} = A^\dag \textbf{b}.$ Show that 
\[
\frac{\norm{\textbf{x}_B - \hat{\textbf{x}}}_2}{\norm{\hat{\textbf{x}}}_2} \leq \norm{R^\inv S}_2.
\]
\partbreak
\begin{solution}
    
\end{solution}


\newpage
\section{Problem 5}
Let $\textbf{u} \in \R^n, \textbf{u}\neq \textbf{0}.$ A \textit{Householder} matrix $H_\textbf{u} \in \R^{n\times n}$ is defined by 
\[
H_\textbf{u} = \id - \frac{2\textbf{u}\textbf{u}^T}{\norm{\textbf{u}}_2^2}
\]
\subsection{Problem 5, part a}
Show that $H_\textbf{u}$ is both symmetric and orthogonal.
\partbreak
\begin{solution}

    \begin{itemize}
        \item \underline{Symmetry:}

        \begin{align*}
            (H_\textbf{u})^T &= (\id - \frac{2}{\norm{\textbf{u}}_2^2}\textbf{u}\textbf{u}^T)^T &\text{(Given.)}\\
            &= \id^T - \frac{2}{\norm{\textbf{u}}_2^2}(\textbf{u}\textbf{u}^T)^T &\text{(Transpose is linear.)}\\
            &= \id - \frac{2}{\norm{\textbf{u}}_2^2}\textbf{u}\textbf{u}^T &\text{($\textbf{u}\textbf{u}^T$ and $\id$ are symmetric.)}\\
            &= H_\textbf{u}
        \end{align*}

        \item \underline{Orthogonality:}
            Note $H_\textbf{u}^T = H_\textbf{u}$, so we just need to show $H_\textbf{u}^2 = \id$.

            \begin{align*}
                H_\textbf{u}^2 &= (\id - \frac{2}{\norm{\textbf{u}}_2^2}\textbf{u}\textbf{u}^T)(\id - \frac{2}{\norm{\textbf{u}}_2^2}\textbf{u}\textbf{u}^T) &\text{(Given.)}\\
                &= \id^2 +\frac{4}{\norm{\textbf{u}}_2^4}\textbf{u}\textbf{u}^T\textbf{u}\textbf{u}^T - \frac{4}{\norm{\textbf{u}}_2^2}\textbf{u}\textbf{u}^T &\text{(Expanding.)}\\
                &= \id +\frac{4}{\norm{\textbf{u}}_2^4}\textbf{u}\norm{\textbf{u}}_2^2\textbf{u}^T - \frac{4}{\norm{\textbf{u}}_2^2}\textbf{u}\textbf{u}^T &\text{(Definition of 2-norm.)}\\
                &= \id +\frac{4}{\norm{\textbf{u}}_2^2}\textbf{u}\textbf{u}^T - \frac{4}{\norm{\textbf{u}}_2^2}\textbf{u}\textbf{u}^T &\text{(Simplifying.)}\\
                &= \id &\text{(Simplifying.)}
            \end{align*}
    \end{itemize}
\end{solution}

\newpage
\subsection{Problem 5, part b}
Show that for any $\alpha \in \R, \alpha \neq 0$,
\[
H_{a\textbf{u}} = H_\textbf{u}
\]
In other words, $H_\textbf{u}$ only depends on the ``direction" of $\textbf{u}$ and not on its ``magnitude".
\partbreak

\begin{solution}

    We will go straight into calculations:

    \alignbreak
    \begin{align*}
        H_{\alpha\textbf{u}} &= \id - \frac{2}{\norm{\alpha\textbf{u}}_2^2}(\alpha\textbf{u})(\alpha\textbf{u})^T &\text{(Given.)}\\
        &= \id - \frac{2}{|\alpha|^2\norm{\textbf{u}}_2^2}\alpha^2(\textbf{u})(\textbf{u})^T &\text{(Transpose is linear and norm definitions.)}\\
        &= \id - \frac{2}{\norm{\textbf{u}}_2^2}(\textbf{u})(\textbf{u})^T &\text{($\alpha^2 = |\alpha|^2$.)}\\
        &= H_\textbf{u} &\text{(Definition.)}
    \end{align*}
    \alignbreak
\end{solution}

\newpage
\subsection{Problem 5, part c}
In general, given a matrix $M \in \R^{n\times n}$ and a vector $\textbf{x}\in \R^n$, computing the matrix-vector product $M\textbf{x}$ requires $n$ inner products - one for each row of $M$ with $\textbf{x}$. Show that $H_\textbf{u}\textbf{x}$ can be computed using only two inner products.
\partbreak
\begin{solution}

    We will go straight into calculations:

    \alignbreak
    \begin{align*}
        H_\textbf{u}\textbf{x} &= (\id - \frac{2}{\norm{\textbf{u}}_2^2}\textbf{u}\textbf{u}^T)\textbf{x} &\text{(Given.)}\\
        &= \id\textbf{x} - \frac{2}{\braket{\textbf{u}}{\textbf{u}}}\textbf{u}\textbf{u}^T\textbf{x} &\text{(2-norm definition and distribution.)}\\
        &= \textbf{x} - \frac{2}{\braket{\textbf{u}}{\textbf{u}}}\textbf{u}\braket{\textbf{u}}{\textbf{x}} &\text{(Inner product definition.)}
    \end{align*}
    \alignbreak

    As we can see we need to just calculate two inner products when doing the matrix-vector product $H_\textbf{u}\textbf{x},$ appearing in the 2-norm of $\textbf{u}$ and the multiplication of the rank-1 matrix $\textbf{u}\textbf{u}^T$.
\end{solution}

\newpage
\subsection{Problem 5, part d}
Given $\textbf{a}, \textbf{b} \in \R^n$ where $\textbf{a} \neq \textbf{b}$ and $\norm{\textbf{a}}_2 = \norm{\textbf{b}}_2.$ Find $\textbf{u} \in \R^n, \textbf{u} \neq 0$ such that
\vspace{-5mm}
\[
H_\textbf{u}\textbf{a} = \textbf{b}.
\]
\vspace{-15mm}
\partbreak
\begin{solution}

    After some guessing and checking, I found $\textbf{u} = \textbf{b} - \textbf{a}$ to work. Here is the proof:
{\small
    \alignbreak
    \vspace{-5mm}
    \begin{align*}
        H_{\textbf{b} - \textbf{a}}\textbf{a} &= \Big(\id - \frac{2}{\norm{\textbf{a} - \textbf{b}}_2^2}(\textbf{b} - \textbf{a})(\textbf{b} - \textbf{a})^T\Big) &\text{(Given.)}\\
        &= \textbf{a} - \frac{2}{\norm{\textbf{b} - \textbf{a}}_2^2}(\textbf{b}\textbf{b}^T\textbf{a} - \textbf{b}\textbf{a}^T\textbf{a} - \textbf{a}\textbf{b}^T\textbf{a} + \textbf{a}\textbf{a}^T\textbf{a}) &\text{(Factoring out.)}\\
        &= \textbf{a}- \frac{2}{\norm{\textbf{b} - \textbf{a}}_2^2}(\textbf{b}\braket{\textbf{b}}{\textbf{a}} - \textbf{b}\norm{\textbf{a}}_2^2 - \textbf{a}\braket{\textbf{b}}{\textbf{a}} + \textbf{a}\norm{\textbf{a}}_2^2) &\text{(Inner product.)}\\
        &= \textbf{a}- \frac{2}{\norm{\textbf{b} - \textbf{a}}_2^2}(\norm{\textbf{a}}_2^2 (\textbf{a} - \textbf{b}) + \braket{\textbf{b}}{\textbf{a}}(\textbf{b} - \textbf{a})) &\text{(Grouping.)}\\
        &= \textbf{a}- \frac{2(\textbf{a} - \textbf{b})}{\norm{\textbf{b} - \textbf{a}}_2^2}(\norm{\textbf{a}}_2^2 - \norm{\textbf{a}}_2^2\cos(\theta_{ab})) &(\braket{\textbf{a}}{\textbf{b}} = \norm{\textbf{a}}\norm{\textbf{b}}\cos(\theta_{ab}).)\\
        &= \textbf{a}- \frac{2\norm{\textbf{a}}_2^2(\textbf{a} - \textbf{b})}{\norm{\textbf{b} - \textbf{a}}_2^2}(1 - \cos(\theta_{ab})) &\text{(Grouping.)}\\
        &= \textbf{a}- \frac{2\norm{\textbf{a}}_2^2 (1 - \cos(\theta_{ab}))(\textbf{a} - \textbf{b})}{\norm{\textbf{b}}_2^2 + \norm{\textbf{a}}_2^2 - 2\braket{\textbf{a}}{\textbf{b}}} &\text{(2-norm definition.)}\\
        &= \textbf{a}- \frac{2\norm{\textbf{a}}_2^2 (1 - \cos(\theta_{ab}))(\textbf{a} - \textbf{b})}{\norm{\textbf{b}}_2^2 + \norm{\textbf{a}}_2^2 - 2\norm{\textbf{a}}_2^2\cos(\theta_{ab})} &(\braket{\textbf{a}}{\textbf{b}} = \norm{\textbf{a}}\norm{\textbf{b}}\cos(\theta_{ab}).)\\
        &= \textbf{a}- \frac{2\norm{\textbf{a}}_2^2 (1 - \cos(\theta_{ab}))(\textbf{a} - \textbf{b})}{2\norm{\textbf{a}}_2^2 - 2\norm{\textbf{a}}_2^2\cos(\theta_{ab})} &(\braket{\textbf{a}}{\textbf{b}} = \norm{\textbf{a}}_2 = \norm{\textbf{b}}_2.)\\
        &= \textbf{a}- \frac{2\norm{\textbf{a}}_2^2 (1 - \cos(\theta_{ab}))(\textbf{a} - \textbf{b})}{2\norm{\textbf{a}}_2^2(1 - \cos(\theta_{ab}))} &\text{(Grouping.)}\\
        &= \textbf{a}- \frac{(1 - \cos(\theta_{ab}))(\textbf{a} - \textbf{b})}{1 - \cos(\theta_{ab})} &\text{(Simplifying.)}\\
        &= \textbf{a} - (\textbf{a} - \textbf{b}) &\text{(Simplifying.)}\\
        &= \textbf{b} &\text{(Simplifying.)}
    \end{align*}
    \vspace{-10mm}
    \alignbreak
}%
\end{solution}

\newpage
\subsection{Problem 5, part e}
Show that \textbf{u} is an eigenvector of $H_\textbf{u}$. What is the corresponding eigenvalue?
\partbreak
\begin{solution}

    We will go straight into calculations:
    \alignbreak
    \begin{align*}
        H_\textbf{u}\textbf{u} &= \Big(\id - \frac{2}{\norm{\textbf{u}}_2^2}\textbf{u}\textbf{u}^T\Big)\textbf{u} &\text{(Given.)}\\
        &= \textbf{u} - \frac{2}{\norm{\textbf{u}}_2^2}\textbf{u}\textbf{u}^T\textbf{u} &\text{(Distribution.)}\\
        &= \textbf{u} - \frac{2}{\norm{\textbf{u}}_2^2}\textbf{u}\norm{\textbf{u}}_2^2 &\text{(2-norm definition.)}\\
        &= \textbf{u} - 2\textbf{u} &\text{(Simplifying.)}\\
        &= -\textbf{u} &\text{(Simplifying.)}
    \end{align*}
    \alignbreak

    So \textbf{u} is an eigenvector of $H_\textbf{u}$ with eigenvalue -1.
\end{solution}

\newpage
\subsection{Problem 5, part f}
Show that every $\textbf{v} \in $ span$\{ \textbf{u}\}^\perp$ is an eigenvector of $H_\textbf{u}$. What are the corresponding eigenvalues? What is dim(span$\{\textbf{u}\}^\perp$)?
\partbreak
\begin{solution}

    We will go straight into calculations:

    \alignbreak
    \begin{align*}
        H_\textbf{u}\textbf{v} &= \Big( \id - \frac{2}{\norm{\textbf{u}}_2^2} \textbf{u}\textbf{u}^T\Big)\textbf{v} &\text{(Given.)}\\
        &= \textbf{v} - \frac{2}{\norm{\textbf{u}}_2^2} \textbf{u}\textbf{u}^T\textbf{v} &\text{(Distribution.)}\\
        &= \textbf{v} - \frac{2}{\norm{\textbf{u}}_2^2} \textbf{u}(0) &(\textbf{v} \in \text{span} (\{ \textbf{u}\}^\perp).)\\
        &= \textbf{v} &\text{(Simplifying.)}
    \end{align*}
    \alignbreak

    So \textbf{v} is an eigenvector of $H_\textbf{u}$ with eigenvalue 1 for all $\textbf{v} \in $ span$\{ \textbf{u}\}^\perp$. Note since $\R^n =$ span$\{ \textbf{u}\}^\perp \oplus $ span$\{ \textbf{u}\}$, then dim($\R^n) =$ dim(span$\{ \textbf{u}\}^\perp$) + span$\{ \textbf{u}\}$. Then dim(span$\{ \textbf{u}\}^\perp$) = $n - 1$.
\end{solution}

\newpage
\subsection{Problem 5, part g}
Find the eigenvalue decomposition of $H_\textbf{u}$, i.e., find an orthogonal matrix $Q$ and a diagonal matrix $\Lambda$ such that 
\[
H_\textbf{u} = Q\Lambda Q^T
\]
\partbreak
\begin{solution}

    Note that we found all eigenvalues and eigenvectors in the previous two parts. Thus $Q$ can be formed by the found eigenvectors of $H_\textbf{u}.$ Therefore, I claim that $Q$ is of the form 

    \[
    Q = \mqty[\vert &\vert &&\vert\\\textbf{u} &\textbf{v}_2&\cdots &\textbf{v}_n\\\vert &\vert &&\vert]
    \]

    We just need to show that $Q$ is orthogonal, that is, $Q$ is composed of orthogonal columns. Note since we showed that every vector in  span$\{ \textbf{u}\}^\perp$ is an eigenvector of $H_\textbf{u}$, we are free to chose any orthogonal basis of $\R^{n - 1}$ (the last dimension is reserved for \textbf{u}). Without loss of generality, choose $\textbf{e}_2, ..., \textbf{e}_{n}$, the set of canonical unit vectors of $\R^{n - 1}$ to be $\textbf{v}_2, ..., \textbf{v}_n$. Note these are all orthogonal to each other, and since they are chosen from span$\{ \textbf{u}\}^\perp$, they are all orthogonal to \textbf{u}. Thus, we can choose 
    \[
    Q = \mqty[\vert &\vert &&\vert\\\textbf{u} &\textbf{e}_2&\cdots &\textbf{e}_n\\\vert &\vert &&\vert]
    \]

    which will be orthogonal. Finally, we choose $\Lambda$ to be the diagonal matrix whose entries coincide with their corresponding eigenvector in $Q$, so $\Lambda = $ diag$(-1, 1, ..., 1) \in \R^{n\times n}$.
\end{solution}

\section{Problem 6}
In this exercise, we will implement and compare Gramâ€“Schmidt and Householder QR. Your implementation should be tailored to the program you are using for efficiency (e.g. vectorize your code in Matlab/Octave/Scilab). Assume in the following that the input is a matrix $A\in \R^{m \times n}$ with rank$(A) = n \leq m$ and we want to find its full $QR$ decomposition, $A = QR$, where $Q \in O(m)$ and $R\in \R^{m \times n}$ is upper triangular.

\newpage
\subsection{Problem 6, part a}
Implement the (classical) Gram-Schmidt algorithm to obtain $Q$ and $R$.
\partbreak
\begin{solution}

    Here is my implementation of Gram-Schmidt in python. I tried to vectorize my code as much as possible. Note that even though $A$ is not ``passed by reference" in python, I avoided doing so much in my code.  

\begin{lstlisting}
def my_qr(A: np.ndarray):
        """
        Returns the QR factorization of a nonsingular array
        via (classical) Gram-Schmidt.

        Parameters
        ----------
        A: np.ndarray
            The matrix with which we find the QR factorization.
            Must be nonsingular, sized n x n

        Returns
        -------
        Q: np.ndarray
            A set of orthonormal column vectors of size n x n

        R: np.ndarray
            An upper triangular matrix
        """
        m, n = A.shape
        Q = np.zeros((m, m))
        R = np.zeros((m, n))

        for i in range(0, m): #Row Iter
            prev = 0 # used to catch r_{jk}q_j in sum
            for j in range(0, i+1): # maintain upper triangularity
                if i != j:
                    R[j, i] = np.dot(Q[:, j], A[:, i])
                    prev += R[j, i] * Q[:, j]
                else: #Diagonal term, take prev
                    R[i, i] = np.linalg.norm(A[:, i] - prev, ord = 2)
                    assert R[i, i] != 0, "Diagonal is zero, function cannot continue"
                    Q[:, j] = (1/R[i, i]) * (A[:, i] - prev)
        return (Q, R)
\end{lstlisting}
\end{solution}

\newpage
\subsection{Problem 6, part b}
Implement the Householder $QR$ algorithm to obtain $Q$ and $R$. You should 
\begin{enumerate}
    \item Store $Q$ implicitly, taking advantage of the fact that it can be uniquely specified by a sequence of vectors of decreasing dimensions;
    \item Choose $\alpha$ in your Householder matrices to have the opposite sign of $x_1$ to avoid cancellation in $v_1$ (cf. notations in lecture notes.)
\end{enumerate}
\partbreak

\begin{solution}

    Here is my code. I believe everything here is vectorized, so it should be suitably fast. I made a simple helper function to calculate the Hausholder matrix, since it seems we'll need to compute it a few times in the next parts.

\begin{lstlisting}
    def calculate_Hausholder(v: np.array):
        """
        Helper function to calculate the Hausholder matrix from v.
        
        Parameters
        ----------
        v : np.array
            The given vector. Needs to be normalized prior to passing.
            
        Returns
        -------
        H : np.ndarray
            The Hausholder matrix of v.
        """
        
        H = np.identity(max(v.shape)) - 2*np.outer(v, v)
        return H

    def hausholder_qr(A: np.ndarray):
    """
        Returns the QR factorization obtained through 
        the Hausholder Reflection method. 
        
        Elements belonging to and above diagonal are R, 
        
        Below the diagonal are the column vectors v_1, ..., v_n-1
        which are used to get Q_1, ..., Q_n-1. 
        
        Last element of v_i is not stored. Can be recovered since 
        v is a unit vector.
    """
    m, n = A.shape
    v_mat = []
    for i in range(m-1):
        
        # Obtaining v_i
        u = A[i:, i].copy()
        u[0] -= np.linalg.norm(u, ord = 2)
        v = u/np.linalg.norm(u, ord = 2)
        v_mat.append(v)
        
        #Calculating Q only to get next iteration
        Q = np.identity(m)
        Q[i:, i:] = calculate_Hausholder(v)
        
        A = np.matmul(Q, A)

    #Storing v_i's
    for i in range(m-1):
        A[i+1:, i] = np.array(v_mat[i])[:-1]
        
    return A
\end{lstlisting}
\end{solution}

\newpage
\subsection{Problem 6, part c}
Implement an algorithm for forming the product $Q\textbf{x}$ and another for forming the product $Q^T\textbf{y}$ when $Q$ is stored implicitly as in (b).
\partbreak
\begin{solution}

    Here is my code. I tested it to make sure the returned matrix is indeed orthonomal. All you need to do to get $Q$ is just call $\textbf{calculate\_Q}$.

\begin{lstlisting}
def recover_Qis(A: np.ndarray):
    """
    Calculates all Q_is from a Hausholder applied matrix A.
    
    Parameters
    ----------
    
    A : np.ndarray
        The matrix on which Hausholder QR has been performed
        
    Returns
    -------
    Qis : list(np.ndarray)
        A list containing all Q's 
    """
    Qis = []
    
    m, n = A.shape
    
    for i in range(n - 1):
        v = np.zeros(m - i)
        v[1:] = A[i+1:, i]
        v[0] = np.sqrt(1 - np.linalg.norm(v, ord = 2)**2)
        Q = np.identity(m)
        Q[i:, i:] = calculate_Hausholder(v)
        Qis.append(Q)
    
    return Qis

def calculate_Q(A: np.ndarray):
    """
    Returns the orthonormal matrix Q from the matrix A after Hausholder QR 
    is applied.
    
    Parameters
    ----------
    
    A : np.ndarray
        The matrix on which Hausholder QR has been performed

    Returns
    -------    
    
    Q: np.ndarray
        The orthonormal matrix involved in QR
    """
    
    Qis = recover_Qis(A)
    Q = np.identity(Qis[0].shape[0])
    for Qi in Qis:
        Q = np.matmul(Q, Qi.T)
        
    return Q
\end{lstlisting}
\end{solution}


\end{document}