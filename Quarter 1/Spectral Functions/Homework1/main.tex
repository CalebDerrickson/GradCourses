%---------
% place your email id between the braces so that your homework has a name
\def\yourname{}
% -----------------------------------------------------
\def\duedate{10/26/2023}
\def\duelocation{via \href{https://www.gradescope.com/courses/635305}{Gradescope}}
\def\hnumber{1}
\def\prof{Lorenzo Orecchia}
\def\course{\href{https://canvas.uchicago.edu/courses/52752}{CMSC 35410 - Autumn
2023}}
%-------------------------------------
\documentclass[10pt]{article}
\usepackage[colorlinks,urlcolor=blue]{hyperref}
\usepackage[osf]{mathpazo}
\usepackage{amsmath,amsfonts,graphicx}
\usepackage{qtree}
\usepackage{latexsym}
\usepackage{subfig}
\usepackage{algpseudocode}
\usepackage[shortlabels]{enumitem}
\usepackage{setspace}
\usepackage{algorithm}
\usepackage{listings}
%\usepackage[top=1in,bottom=1.4in,left=1.5in,right=1.5in,centering]{geometry}
\usepackage{fullpage}
\usepackage{color}
\usepackage{bbm}
\usepackage{pgffor}
\foreach \x in {a,...,z}{%
\expandafter\xdef\csname v\x\endcsname{\noexpand\ensuremath{\noexpand\mathbf{\x}}}
}
\foreach \x in {A,...,Z}{%
\expandafter\xdef\csname v\x\endcsname{\noexpand\ensuremath{\noexpand\mathbf{\x}}}
}
\foreach \x in {A,...,Z}{%
\expandafter\xdef\csname m\x\endcsname{\noexpand\ensuremath{\noexpand\mathbf{\x}}}
}
\newcommand{\1}{\vec{\mathbbm{1}}}
\definecolor{mdb}{rgb}{0.3,0.02,0.02}
\definecolor{cit}{rgb}{0.05,0.2,0.45}
%\pagestyle{myheadings}
\markboth{\yourname}{\yourname}
\thispagestyle{empty}
\newenvironment{proof}{\par\noindent{\it Proof.}\hspace*{1em}}{$\Box$\bigskip}
\newcommand{\qed}{$\Box$}
\newcommand{\alg}[1]{\mathsf{#1}}
\newcommand{\handout}{
\renewcommand{\thepage}{H\hnumber-\arabic{page}}
\noindent
\begin{center}
\vbox{
\hbox to \columnwidth {\sc{\course} --- \prof \hfill}
\vspace{-2mm}
\hbox to \columnwidth {\sc due \MakeLowercase{\duedate} \duelocation\hfill {\Huge\color{mdb}H\hnumber.\yourname}}
}
\end{center}
\vspace*{2mm}
}
\newcommand{\solution}[1]{
\vspace{2mm}
\noindent Collaborators: I have Alex Sobczynski and Chad Schmurling to thank for their collaboration with this homework. The majority of collaboration was done under problem 4. The rest of the problems were discussed, but not directly aided by Alex or Chad. The materials I mainly used were the book by Daniel A. Spielman for problem 2 and 3, and various online exercises to help me wade through problem 4.

\vspace{5mm}
\medskip\noindent{\color{cit}\textbf{Solution:} #1}}
\newcommand{\bit}[1]{\{0,1\}^{ #1 }}
\newcommand{\extraspace}{\medskip\noindent{\color{cit} Extra space for your
solution}\newpage}
%\dontprintsemicolon
%\linesnumbered
\newtheorem{problem}{\sc\color{cit}Problem}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{claim}{Claim}
\begin{document}
\handout
\begin{itemize}
\item The assignment is due at Gradescope on \duedate.
\item A LaTex template will be provided for each homework. Type your homework into
this template. This will help facilitate the grading.
\item You are permitted to discuss the problems with up to 2 other students in the
class (per problem); however, {\em you must write up your own solutions, in your
own words}. Do not submit anything you cannot explain. If you do collaborate with
any of the other students on any problem, please list all your collaborators in the
appropriate spaces.
\item Similarly, please list any other source you have used for each problem,
including other textbooks or websites.
\item {\em Show your work.} Answers without justification will be given little
credit.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{problem}[Tracking convergence (6 points)]
In order to show the convergence of the Heat Equation, we tracked the Dirichlet
energy $\mathcal{U}(\vx_t) = \frac{1}{2}\|\vx_t - \bar{x} \1\|_{\mD_G}^2$, where $\bar{x} = \frac{\1^T\mD_G \vx_0}{vol(V)}$.
\begin{enumerate}[(a)]
\item Show that $\bar{x} = \arg \min_{u} \mathcal{U}(\vx_t)$.
\item Show that $\mathcal{U}(\vx_t) = \frac{1}{2}\vx_t^T \mL_{K_G} \vx_t$,
where $K_G$ is the complete graph with the same degrees as $G$, meaning $w_{ij}
(K_G) = \frac{d_i d_j}{vol(V)}$.
\end{enumerate}
\end{problem}

\solution{ 

Note for this problem, I will shorthand $\Bar{\textbf{x}} = \Bar{x} \1$ as $\bar{x}$.
{\setstretch{1.5}

\begin{enumerate}[(a)]
    \item We first wish to show $U(\bar{x}) \in \min \Big\{ \frac{1}{2} \lVert x - \bar{x} \rVert^2_{D_G} \Big\}$. Note that we only need to look at the value $\min \Big\{ \lVert x - \bar{x} \rVert_{D_G} \Big \}$, since their minima is equivalent. By positive definiteness of the $D_G$ norm, its minimum is zero. Then, $U(\bar{x}) = \lVert \Bar{x} - \Bar{x} \rVert_{D_G} = 0$ for $\bar{x} \in \mathbb{R}^V$. Therefore $U( \bar{x}) \in \min U$, so $\bar{x} \in \arg \min U(x_t)$.
    
    \vspace{5mm}
    Next we wish to show that \textit{only} $\bar{x}$ achieves the minimum of $U(x)$. Note also by positive definiteness of $\lVert \cdot \rVert_{D_G}$, $U$ is minimized if and only if $x = \bar{x}$. Then the only $x \in \mathbb{R}^V$ which achieves the minimum of $U$ is $\bar{x}$. Thus $ \bar{x} = \arg \min U(x_t)$.

    \item I do not exactly understand what it means for the complete graphs to have the same degrees as the original graph in this context. Even so, I will write out as far as I got. This is almost all algebra and examining terms appearing from the norm of the vector $(\textbf{x}_t - \Bar{x}\1)$.

    \begin{align}
        U(x_t) &= \frac{1}{2} \lVert \textbf{x}_t - \Bar{x}\1\rVert ^2_{D_G} &\text{(Given.)}\nonumber\\
        &= \frac{1}{2} (\textbf{x}_t - \Bar{x}\1)^TD_G(\textbf{x}_t - \Bar{x}\1) &\text{(Definition of norm.)}\nonumber\\
        &= \frac{1}{2} \Big[ \textbf{x}_t^TD\textbf{x}_t + \Bar{x}^2 \1^T D\1 - \Bar{x}\textbf{x}_t^TD\1 - \Bar{x} \1^TD\textbf{x}_t\Big] &\text{(Factoring out.)}\nonumber\\
        &= \frac{1}{2}\Big[ (1) + (2) - (3)\Big] &\text{(Labelling terms.)}\nonumber\\
        1): \ &\textbf{x}_t^T D \textbf{x}_t &\text{(Given.)}\nonumber\\
        &= \sum_{i = 1}^nd_i \textbf{x}_{t, i}^2 &\text{(Working out terms.)}\nonumber\\
        2): \ &\Bar{x}^2 \sum_{i \in V} d_i &\text{(By (2).)}\nonumber\\
        &= \Bar{x}^2 Vol(V) &\text{(Definition.)}\nonumber\\
        3): \ &\Bar{x}\textbf{x}_t^TD\1 - \Bar{x} \1^TD\textbf{x}_t &\text{(Given.)}\nonumber\\
        &= \Bar{x}\Big( \sum_{i = 1}^n d_i \textbf{x}_{t, i} + \sum_{i = 1}^n d_i \textbf{x}_{t, i}\Big) &\text{(Working out terms.)}\nonumber\\
        &= 2\Bar{x}\sum_{i = 1}^n d_i \textbf{x}_{t, i} &\text{(Simplifying.)}\nonumber\\
        \implies U(\textbf{x}_t) &= \sum_{i = 1}^nd_i \textbf{x}_{t, i}^2 + \Bar{x}^2 Vol(V) - 2\Bar{x}\sum_{i = 1}^n d_i \textbf{x}_{t, i} &\text{(Gathering terms.)}\nonumber
    \end{align}

    \textit{If} we are to assume that the degrees of the graph are constant, then the $d_i's$ in the summation would be constant, thus each term would be a multiple of $Vol(V)$. I don't believe we can do that, and aside from applying the definition of $\Bar{x}$, I'm afraid this is as far as I could get. 

\end{enumerate}
}

}
\newpage
\begin{problem}[Graph Comparison (30 points)]
For this problem we will define that $G \succeq c \cdot H \Leftrightarrow \forall
k, \lambda_k(G) \geq c \cdot \lambda_k(H)$.
\begin{enumerate}[(a)]
\item Prove $(n - 1) \cdot P_n \succeq G_{1,n}$, where $P_n$ is the path from
vertex $1$ to vertex $n$, and $G_{1, n}$ is the graph with just the edge $\{1, n\}
$.
\item Show that $\forall 2 \leq j \leq n, \lambda_j(K_n) = 1 + \frac{1}{n-1}$,
where $K_n$ is the clique on $n$ vertices.
\item The complete graph $K_n$ can be decomposed to all its edges. Notice that
$K_n = \sum_{1 \leq i < j \leq n} G_{i,j}$. Prove that
$$K_n \preceq \sum_{1 \leq i < j \leq n} (j - i) P_n$$
\item Conclude that $P_n \succeq O(1/n^2) K_n \Rightarrow \lambda_2(P_n) \geq
O(1/n^2)$
\item Show that $\forall 2 \leq j \leq n, \lambda_j\left(L(K_G), \mD_G\right) =
1$, where $K_G$ is the complete graph with the same degrees as $G$, meaning $w_{ij}
(K_G) = \frac{d_i d_j}{vol(V)}$.
\end{enumerate}
Combining with the upper bound proof of $\lambda_2(P_n)$ we can conclude that
$\lambda_2(P_n) = \Theta(1/n^2)$.
\begin{enumerate}[(a),resume]
\item Follow the same practice method to prove $\frac{2}{(n-1)} \leq \lambda_2(T_d) \geq \frac{1}{(n - 1) \log_2(n)}$, where $T_d$ is the {\bf complete
binary} tree.
\end{enumerate}
\end{problem}
\solution{

\begin{enumerate}[(a)]
    \item The given statement is equivalent to showing for any $\textbf{x} \in \mathbb{R}^V, (n - 1)\textbf{x}^TL_{P_n}\textbf{x} \geq \textbf{x}^TL_{G_{1, n}}\textbf{x}$. Note the quadratic form of $\textbf{x}^TL_{G}\textbf{x}$, for any unweighted graph, is given by 
    \[
    \textbf{x}^TL_G\textbf{x} = \sum_{(u, v) \in E} (\textbf{x}(u) - \textbf{x}(v))^2.
    \]

    Thus, for our graphs in particular, 
    \[
    \textbf{x}^TL_{P_n}\textbf{x} = \sum_{(u, v) \in E} (\textbf{x}(u) - \textbf{x}(v))^2
    \hspace{5mm}
    \textbf{x}^TL_{G_{1, n}}\textbf{x} = \sum_{(u, v) \in E} (\textbf{x}(u) - \textbf{x}(v))^2
    \]

    Note that the edges we are summing over are different for both graphs. In the case for the path graph, edges come in the form $(i, i+1), (i+1, i)$ for $0 < i \leq n$. Note since each case will add the same amount to the sum, we can git rid of one of these edges and multiply the sum bay a factor of two. This also shows up in the $G_{1, n}$ graph, so we can just throw out the factor of two in both summations when comparing. Therefore, the given statement is equivalent to showing
    \[
    (n - 1)\sum_{u = 1}^{n - 1}(\textbf{x}(u+1) - \textbf{x}(u))^2 \geq (\textbf{x}(n) - \textbf{x}(1))^2.
    \]

    We can note that, via telescoping, we can rewrite the right hand side as
    \[
    (\textbf{x}(n) - \textbf{x}(1))^2 = (\textbf{x}(n) - \textbf{x}(n-1) + \textbf{x}(n-1) - \textbf{x}(n-2) + ... + \textbf{x}(2) - \textbf{x}(1))^2 = \Big(\sum_{u = 1}^{n-1} x(u+1) - x(u)\Big)^2  
    \]

    Let us then denote $\Delta(u) = \textbf{x}(u+1) - \textbf{x}(u) \in \mathbb{R}^{n-1}$. Then, the right hand side is equivalent to 
    \[
    \Big(\sum_{u = 1}^{n-1} x(u+1) - x(u)\Big)^2 = \Big(\sum_{u = 1}^{n-1}\Delta(u)\Big)^2. 
    \]

    An equivalent way to rewrite a summation is to multiply it by the $\1$ vector. Then, 
    \[
    \Big(\sum_{u = 1}^{n-1}\Delta(u)\Big)^2 = (\1_{n-1}^T\boldsymbol{\Delta})^2
    \]

    We can then apply Cauchy-Schwartz to write, 

    \[(\1_{n-1}^T\boldsymbol{\Delta})^2 \leq (\lVert \1_{n-1} \rVert \lVert \boldsymbol{\Delta} \rVert)^2 = (n-1) (\lVert \boldsymbol{\Delta} \rVert)^2
    \]

    But the norm of $\boldsymbol{\Delta}$ squared is just the sum of the squares of the components of $\boldsymbol{\Delta}$. Thus, 

    \[
    (\textbf{x}(n) - \textbf{x}(1))^2 \leq (n-1)\sum_{u = 1}^{n - 1}(\textbf{x}(u+1) - \textbf{x}(u))^2
    \]

    which is what we wanted to show. \hfill $\square$

    \item Note that the clique of a graph $K_n$ is a set of X vertices of a graph $G$ with the property that every pair of distinct vertices in $X$ are adjacent in $G$. By this definition, any clique is a complete subgraph of $G$. Consequences of being a complete graph are that (for unweighted graphs with no self-connecting nodes), its adjacency matrix equals the all-ones matrix $\mathbb{J}$ minus the identity matrix $\mathbb{I}$. Also, since every pair of nodes is connected, the degree of each node is equal to $(n - 1)$. Thus $D = (n-1)$ diag($\1$). Note that the form of the eigenvalues we are looking for are only found in the normalized Laplacian of $K_n$, Thus, we can write the following:

    \begin{align}
        \mathbb{L} &= D^{-1/2}(L)D^{-1/2} &\text{(Given.)}\nonumber\\
        &= D^{-1/2}(D-A)D^{-1/2} &\text{(Another from of the Laplacian.)}\nonumber\\
        &= D^{-1/2}(D)D^{-1/2} -D^{-1/2}(A)D^{-1/2} &\text{(Distributivity.)}\nonumber\\
        &= \mathbb{I} - D^{-1/2}(A) D^{-1/2} &\text{(Diagonal matrices will cancel.)} \nonumber\\
        &= \mathbb{I} - D^{-1/2}(\mathbb{J} - \mathbb{I}) D^{-1/2} &\text{(By above.)}\nonumber\\
        &= (1 - \frac{1}{n-1})\mathbb{I} - D^{-1/2}(\mathbb{J})D^{-1/2} &\text{(Commutivity of $\mathbb{I}$ and grouping.)}\nonumber\\
        &= (1 - \frac{1}{n-1})\mathbb{I} -\frac{1}{n-1}\mathbb{J} &\text{(Simplification.)}\nonumber\\
        &= \mathbb{I} + \frac{1}{n-1}(\mathbb{J} - \mathbb{I}) &\text{(Rearranging.)}\nonumber\\
        &= \mathbb{I} - \frac{1}{n-1}A &\text{(For complete graphs.)}\nonumber
    \end{align}

    Thus, we seek to find the eigenvalues of the following matrix
    \[
    \mathbb{L} = 
    \begin{pmatrix}
    1 &-\frac{1}{n-1} &\dots &-\frac{1}{n-1}\\
    -\frac{1}{n-1} &1 &\dots &-\frac{1}{n-1}\\
    \vdots &&\ddots &\vdots\\
    -\frac{1}{n-1} &\dots &-\frac{1}{n-1} &1
    \end{pmatrix}
    \]

    Instead of directly calculating the eigenvalues, we can seek to find them using some intuition. If we take into the consideration the all-ones vector $\1$, we can see, when this matrix is applied, each component of the resulting vector will equal $1  - \frac{1}{n-1}(n-1)$, which will equal $0$. Thus we will get the zero-vector, which means $\1$ is an eigenvector of $\mathbb{L}$ with eigenvalue 0. If we introduce a vector $\textbf{v}_j = [-1, 0, ..., 1, 0, ..., 0]$ (this is a vector with nonzero entries only in the first and $j-th$ component) and apply $\mathbb{L}$, we can notice the first component will equal $-1 - \frac{1}{n-1}$ for any choice $j$ and the $j$-th component will equal $\frac{1}{n-1} + 1$. \par 
    
    \vspace{5mm}
    Thus we notice $\mathbb{L}\textbf{v}_j = (1 + \frac{1}{n-1})\textbf{v}_j$ for any $2 \leq j \leq n$. So $\textbf{v}_j$ is an eigenvector of $\mathbb{L}$ with eigenvalue $1 + \frac{1}{n-1}$. We can note that we have thus found $n$ linearly independent eigenvectors, so no additional eigenvalues can be observed. Thus, 0 is the smallest eigenvalue with all other eigenvalues equaling $1 + \frac{1}{n-1}$, which is what we wanted to find. \hfill $\square$  

    \item Note that if we find $G_{ij} \preceq (j - i) P_n$, then by summing over $j, i$ with $j > i$, we can find the given statement to be true. Since a similar method was done in part a, we will take some queues from it. Thus, the following steps are justified:

    \begin{align}
        &\textbf{x}^TL_{G_{ij}}\textbf{x}\nonumber\\ &= \sum_{(u, v) \in E} (\textbf{x}(u) - \textbf{x}(v))^2 &\text{(Given.)}\nonumber\\
        &= (\textbf{x}(i) - \textbf{x}(j))^2 &\text{($E = \{(i, j)\}$ for $G_{ij}, j > i$.)}\nonumber\\
        &= (\textbf{x}(i) - \textbf{x}(i+1) + \textbf{x}(i+1) - \textbf{x}(i+2) + ... + \textbf{x}(j-1) + \textbf{x}(j))^2 &\text{(Adding zeros in form $\textbf{x}(k) - \textbf{x}(k)$.)}\nonumber\\
        &\leq \Big( \sum_{k = i}^j\textbf{x}(k) - \textbf{x}(k+1)\Big)^2 &\text{(Triangle inequality.)}\nonumber\\
        &= \Big( \sum_{k = i}^j \boldsymbol{\Delta}\Big)^2 &(\boldsymbol{\Delta} := \textbf{x}(k) - \textbf{x}(k+1).)\nonumber\\
        &= (\1_{j - i}^T\boldsymbol{\Delta})^2 &\text{(Another way to sum over vector.)}\nonumber\\
        &\leq \lVert \1_{j - i}^T\rVert^2 \lVert \boldsymbol{\Delta}\rVert ^2 &\text{(Cauchy-Schwartz.)}\nonumber\\
        &= (j - i)\sum_{k = i}^j (\textbf{x}(k+1) - \textbf{x}(k))^2 &\text{(Taking 2-norm.)}\nonumber\\
        &\leq (j - i)\sum_{k = 1}^{n-1} (\textbf{x}(k+1) - \textbf{x}(k))^2 &\text{(Adding positive terms.)}\nonumber\\
        &= (j - i) \textbf{x}^TL_{P_n}\textbf{x} &\text{(Definition.)}\nonumber
    \end{align}

    Thus, by the given equality of complete paths, summing over $i, j$ with $j > i$ will yield the desired inequality. \hfill $\square$

    \item We just showed in the previous part that $K_n \preceq \sum_{i < j} P_n$. So the largest eigenvalue of $K_n$ is less than or equal to the largest eigenvalue of $P_n$ times the summation. Note the summation will run over all of $1 \leq i < j \leq n$, thus the summation runs in $O(n^2)$ time. The following is then justified:

    \begin{align}
        \lambda_2(K_n) &\leq O(n^2)\lambda_2(P_n) &\text{(Reasoned above.)}\nonumber\\
        1 +\frac{1}{n - 1} &\leq O(n^2)\lambda_2(P_n) &\text{(By part b.)}\nonumber\\
        \implies \frac{1}{O(n^2)} + \frac{1}{(n - 1)O(n^2)}& \leq \lambda_2(P_n) &\text{(Dividing by $O(n^2)$.)}\nonumber\\
        \implies \frac{1}{O(n^2)} & \leq \lambda_2(P_n) &\text{(Term dropped is positive, so is upper bound.)}\nonumber
    \end{align}

    Thus, if we can take $1/O(n^2) = O(1/n^2)$, then the statement has been proven. \hfill $\square$

    \item I'm not entirely sure on what it means for the clique $K_n$ to maintain the same degrees as the original graph. I unfortunately couldn't finish this part in time. 

    \item To construct these bounds on $\lambda_2(T_d)$, we need to construct some reasonable test vector on the tree. I tried my best to create a binary tree in Latex for a visual example. 

    \Tree [.1 [.2 4 5 ] [.3 6 7 ]   ] 

    \vspace{5mm}
    Let's construct our test vector $\textbf{v}(a)$ as \textbf{v}(1) = 0, \textbf{v}(2) = 1, and \textbf{v}(3) = -1. From this, we'll take every vertex which can be reached by 2 without going through 1 to have value 1, otherwise the node will have value zero. This is what our tree will look like for our text vector.
    \vspace{5mm}

    \Tree [.$\textbf{v}(1)=0$ [.$\textbf{v}(2)=1$ $\textbf{v}(4)=1$ $\textbf{v}(5)=1$ ] [.$\textbf{v}(3)=-1$ $\textbf{v}(6)=-1$ $\textbf{v}(7)=-1$ ]   ] 

    \vspace{5mm}
    As it turns out, our test vector is symmetric, so when summing over all nodes in the tree, $\1^T\textbf{v} = 0$. Note the only term that survives in our ``energy" is are the edges connecting $\textbf{v}(1), \textbf{v}(2)$ and $\textbf{v}(3)$. Thus we can write

    \[
    \lambda_2 \leq \frac{\textbf{v}^TL\textbf{v}}{\textbf{v}^T\textbf{v}} = \frac{\sum_{a\sim b} (\textbf{v}(a) - \textbf{v}(b))^2}{\sum_a \textbf{v}(a)^2} = \frac{(\textbf{v}(1) - \textbf{v}(2))^2 + (\textbf{v}(1) - \textbf{v}(3))^2}{n - 1} = 2/(n - 1)
    \]

which gives us the upper bound on $\lambda_2$. To get a lower bound, let us construct a path which will run from some vertex $a$ to another vertex $b$. This path will the be denoted by $T_d^{ab}$, and will have at most a length $2d$, since a max length in our example would be from vertex $4$ to vertex $7$. As we are working over a balanced binary tree, the depth $d$ is equal to $\log_2 n$. This will not necessarily hold, however, when the tree is not balanced. The depth will then be bounded by $\log_2 n$ in this case. Thus, th length of the longest path, $2d$, is bounded by $2 \log_2 n$. We can then write the following:

\[
K_n = \sum_{a < b}G_{a, b} \preceq \sum_{a < b} (2d)T_d^{ab} \preceq \sum_{a < b} (2\log_2 n)T_d = {n \choose 2}(2\log_2 n)T_d = n(n-1)(\log_2 n)T_d
\]
\end{enumerate}

By part e, we showed for a complete graph, keeping the same degrees as the original graph, that $\lambda_2 = 1$. This then means 
\[
1 = \lambda_2(K_n) \leq n(n-1)(\log_2n)\lambda_2(T_d)
\]

The n can be discarded, since we know that $(n-1)(\log_2n)\lambda_2(T_d)\geq 1$ for $n > 1$. We then get, by rearranging,

\[
\frac{1}{(n-1)\log_2 n} \leq \lambda_2(T_d)
\]

which is what we wanted to find. \hfill $\square$
}
\newpage
\begin{problem}[Spectrum of bipartite graphs (10 points)]
Show that
$$G\text{ is bipartite } \Leftrightarrow \forall \lambda \in \text{spec}(\mA_G) \rightarrow -\lambda \in \text{spec}(\mA_G)$$
In words, you need to show that if the eigenvalues of the adjacency matrix $\mA_G$
appear in pairs of $\lambda$ and $-\lambda$, then graph $G$ is bipartite and vice
versa.
\end{problem}
\solution{

We will first show the forward implication. That is, given $G$ is bipartite, we need to show that if $\lambda$ is an eigenvalue of the adjacency matrix $A$, then so is $-\lambda$. First, let $\textbf{v}$ be the associated eigenvector of $\lambda$. Since we are given $G$ is bipartite, we can orient the graph such that there are two sets of vertices $S$ and $T$ $\subset V$ which partition $V$. Let us then define a new vector $\textbf{x}$ as 
\[
\textbf{x}_a = 
\begin{cases}
\ \textbf{v}_a   &\text{If $a \in S$,}\\
-\textbf{v}_a   &\text{If $a \in T$}
\end{cases}
\]

for some vertex $a \in V$. I claim that $\textbf{x}$ is an eigenvector of $G$ with eigenvalue $-\lambda$. To show this, observe the following:

\vspace{-15mm}
{\setstretch{2}
\begin{align}
    (A\textbf{x})_a &= \sum_{(a, b)\in E} A_{ab}x_b &\text{(For some $a \in S, b \in T$.)}\nonumber\\
    &= \sum_{(a, b)\in E} A_{ab}(-\textbf{v}_b) &\text{(Definition of $\textbf{x}$.)}\nonumber\\
    &= -\lambda\textbf{v}_a &\text{(\textbf{v} is an eigenvalue of $A$.)}\nonumber\\
    &= -\lambda\textbf{x}_a &\text{(Definition of \textbf{x}.)}\nonumber
\end{align}
}%

Note if $a \in T$, a similar equality will be found. 

\vspace{-15mm}
{\setstretch{2}
\begin{align}
    (A\textbf{x})_a &= \sum_{(a, b)\in E} A_{ab}x_b &\text{(For some $a \in T, b \in S$.)}\nonumber\\
    &= \sum_{(a, b)\in E} A_{ab}(\textbf{v}_b) &\text{(Definition of $\textbf{x}$.)}\nonumber\\
    &= \lambda\textbf{v}_a &\text{(\textbf{v} is an eigenvalue of $A$.)}\nonumber\\
    &= -\lambda\textbf{x}_a &\text{(Definition of \textbf{x}.)}\nonumber
\end{align}
}

Thus, $\textbf{x}$ is an eigenvector of $A$ with eigenvalue $-\lambda$, proving the forward implication. \par

\newpage
For the backward implication, we are given that for every eigenvalue $\lambda$ of $A$, there will be a negative eigenvalue $-\lambda$ of $A$, and are tasked to show that the graph $G$ is bipartite. Assume that $\lambda$ is the largest eigenvalue of $A$. Thus by assumption let, $\mu = -\lambda$, so $\mu$ is the smallest eigenvalue of $A$. Let \textbf{v} be an eigenvector of $A$ with associated eigenvalue $\mu > 0$. Without loss of generality, assume that $\textbf{v}^t\textbf{v} = 1$, meaning it is a unit eigenvector, since if it wasn't we can choose $\alpha \in \mathbb{R}$ such that $(\alpha\textbf{v})^t\alpha\textbf{v} = |\alpha|^2 \textbf{v}^t\textbf{v} = 1$.  Define a new vector \textbf{u} $\in \mathbb{R}^V$ such that $u_a = |z_a|$ for all $a \in V$. Then by the Rayleigh quotient of \textbf{v}, the following can be justified:

\vspace{-15mm}
{\setstretch{2}
\begin{align}
    |\mu| &= |\textbf{v}^tA\textbf{v}| &\text{(Rayleigh quotient of \textbf{v}.)}\nonumber\\
    &= |\sum_{(a, b) \in E} A_{ab}\textbf{v}_a\textbf{v}_b| &\text{(Matrix vector multiplication.)}\nonumber\\
    &\leq \sum_{(a, b) \in E} |A_{ab}\textbf{v}_a\textbf{v}_b| &\text{(Triangle Inequality.)}\nonumber\\
    &\leq \sum_{(a, b) \in E} A_{ab}|\textbf{v}_a||\textbf{v}_b| &\text{(Cauchy Schwartz.)}\nonumber\\
    &= \sum_{(a, b) \in E} A_{ab}\textbf{u}_a\textbf{u}_b &\text{(Definition of \textbf{u}.)}\nonumber\\
    &= \textbf{u}^tA\textbf{u} &\text{(Matrix vector multiplication.)}\nonumber\\
    &\leq \max_x \frac{\textbf{x}^tA\textbf{x}}{\textbf{x}^t\textbf{x}} &\text{(Max definition.)}\nonumber\\
    &= \lambda &\text{(Given.)}\nonumber
\end{align}
}

Note that $|\mu| = \lambda$, thus equality holds for all intermediate steps. In particular, we can note that $\textbf{u}$ is an eigenvector of $A$ with eigenvalue $\lambda$, and $\textbf{v}^tA\textbf{v} \leq 0$. Observing the third and fifth steps above, we notice that the sum of \textbf{u} over all edges is equal to the absolute sum of \textbf{v} over all edges, we can say that $A_{ab}\textbf{v}_a \textbf{v}_b$ for all edges $(a, b)$, since if it weren't, then one edge would contribute positively to the sum, which this term could be replaced with zero to get an even smaller sum, which cannot happen, since \textbf{v} was chosen to correspond with the smallest eigenvalue of $A$. Thus, $\textbf{v}_a \textbf{v}_b \leq 0$ for all edges $(a, b) \in E$. Thus, at some nodes for each node we take either $\textbf{v}_a$ positive or $\textbf{v}_b$ positive, since they cannot have the same sign. We can then group these vertices into sets based on this classification, i.e.

\[
S = \{ a : \textbf{v}_a < 0\}, \hspace{5mm} T = \{ a : \textbf{v}_a > 0\}
\]

Therefore we have shown two disjoint sets can be constructed from the vertices of $V$, with edges appearing only between the two sets. Thus $G$ is bipartite. \hfill $\square$
}
\newpage
\begin{problem}[{\tt NetworkX} and practical applications (40 points)]
Download {\tt Homework 1 - Networkx, Heat Equation, Spectral Embedding.ipynb}
Jupyter Notebook from Canvas and complete the instructions inside. In order to
complete this question your choices are to
\begin{enumerate}
\item Install Jupyter Notebooks locally
\item Use Google Colab
\item Use one of the CSIL computers that already has Jupyter Notebooks
installed
\end{enumerate}
If you are missing any libraries, install them using `pip install [library]`. You
should upload the finished notebook on Gradescope {\bf in the separate programming
assignment}.
If you have any questions, come to Konstantinos' office hours or send an email to
the instructors.
\end{problem}
\end{document}
